[["6-Chapter6.html", "6 Inferential statistics", " 6 Inferential statistics These are hot peppers. Like hot peppers, statistics can cause pain and heartburn if you are not accustomed to them. Ready or not, lets get munching!! This week we will begin conducting our first statistical tests! We are going to start small and simple, and we will build complexity during the remainder of the semester. We will also start to make more use of some of the programming techniques that you have been developing, and we will build a foundation for moving into regression models in coming weeks. Well start with some simple methods for testing hypotheses about sampling distributions this week. Although relatively limited in scope within the fields of biology and ecology, these tend to be fairly robust tests, and can be powerful tools if studies are designed thoughtfully. For this week, we will focus on implementation of one-sample t-tests, two-sample t-tests, Wilcox tests, and frequency analysis using a \\(\\chi^2\\) test. Within the context of the assumptions of these tests we will also discuss the F-test and the Shapiro-Wilk test of normality. In short, you probably will learn more statistical tests in this preliminary chapter about statistical inference than you have in your college career to this point. Take your time and soak in all the mathy goodness. Well need it! For this Chapter, we will continue working with packages from the tidyverse. You can go ahead and put this in the top of your code for the chapter if you want to load it all at once: library(tidyverse) We will also need the grass carp data for this exercise, which we will load from grasscarp.csv. Remember that you can download all of the class data here or you can get the individual grasscarp.csv file by clicking here and saving with Ctrl + S (Windows) or Command + S (Mac OS-X). These data come from a long-term study of fish population responses to changes in their primary food source, the invasive hydrilla (Hydrilla verticallata). There are a whole bunch of columns in here! The important variables for this chapter are Year (year of fish collection), Age (the age of each fish), Length (total length of fish in mm), and hydrilla (hectares of hydrilla measured each Year). "],["6.1-one-sample-tests.html", "6.1 One-sample tests", " 6.1 One-sample tests Sometimes, we are interested in simply knowing whether or not the measurements weve obtained from an individual or a group are representative of a larger population. For example, we may have a control group in an experiment and we want to know if the group is truly representative of the population average or some measurement we have collected from a different biological population. For these situations, we will rely on one-sample tests this week and well look at other (totally related) options moving forward. 6.1.1 One sample t-test We will examine parametric and non-parametric examples of one-sample tests here to demonstrate why and how we use them. Lets start with a simple example of how we might do this, and what the results actually mean. Well use some data from grass carp (Ctenopharyngodon idella) from Lake Gaston, Virginia and North Carolina, USA for this example. We will compare the size of grass carp at specific ages with their population density using a few different tools Read in the data set: grasscarp &lt;- read.csv(&#39;data/grasscarp.csv&#39;) Just for funsies, you could also read this in directly from the link to the raw data in the GitHub repository for this book if you have an internet connection: grasscarp &lt;- read.csv(&#39;https://raw.githubusercontent.com/danStich/worst-r/master/data/grasscarp.csv&#39;) Remember to check out the data set in your Environment tab so you understand how many observations there are and how many variables (as well as their types). Lets start by asking a simple biological question: is the size of age-3 grass carp different from the average size of fish in this population? First, lets create a sample that includes only age-3 fish. We will store this to a new vector called age3_lengths. age3_lengths &lt;- grasscarp$Length[grasscarp$Age == 3] Now, lets compare the Length of age-3 fish to the rest of the population using a one-sample t-test. To do this, we need to pass age3_lengths to the t.test() function as our observed x. Well also specify that we want to compare the observed sample to the population mean (mu) that we specify on the fly. We will tell R to use a default confidence level (conf.level) of 95%. Finally, we will save the output of our test to a new object, creatively named our_test. # Run the test and save the output to an object our_test = t.test(age3_lengths, mu = mean(grasscarp$Length), conf.level = 0.95 ) # Print the results of the object to the console print(our_test) ## ## One Sample t-test ## ## data: age3_lengths ## t = -18.829, df = 47, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 973.0118 ## 95 percent confidence interval: ## 749.2547 792.4536 ## sample estimates: ## mean of x ## 770.8542 Okay, so what does that mean??? First, lets look at what weve done here. Weve conducted a one-sample t-test. The null hypothesis was that: (H0): the sample (age-3 fish) did not differ in Length from the mean of the population. This is because we stated no specific alternative hypothesis when we executed the t-test above. If we had used a different alternative hypothesis (i.e. greater or less in the argument alternative) then our null would be formalized as: The length of age-3 fish is not significantly greater (or less) than the population mean. Finally, we specified the confidence level. Here, we are told R that we want to know the result with a confidence level of 95% (0.95). This corresponds to a Type-I error rate (\\(\\alpha\\)) of 0.05. This means we are looking for p &lt; 0.05 to conclude that the sample is statistically different from the population mean. Since p &lt; 0.001, we reject the H0 and conclude that age-3 fish are significantly shorter than the population mean. 6.1.1.1 Output R returns the output of statistical tests as objects, and you can reference any part of those objects by name or index. The type of object that R returns, and how you access the parts depends on the type of test you ran and with what options. Like so many other models objects, our one sample t-test is stored as a list: str(our_test) ## List of 10 ## $ statistic : Named num -18.8 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 47 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 1.57e-23 ## $ conf.int : num [1:2] 749 792 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num 771 ## ..- attr(*, &quot;names&quot;)= chr &quot;mean of x&quot; ## $ null.value : Named num 973 ## ..- attr(*, &quot;names&quot;)= chr &quot;mean&quot; ## $ stderr : num 10.7 ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;One Sample t-test&quot; ## $ data.name : chr &quot;age3_lengths&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; We can just look at the names if there are specific pieces in which we are interested. For example, we might want to save the p-value (p.value): # Shows us the names of the things inside the model list names(our_test) ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; ## [6] &quot;null.value&quot; &quot;stderr&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; # This stores our p-value to an object for use p_out = our_test$p.value # And of course we can look at it print(p_out) ## [1] 1.572767e-23 Now we can go through the output as it is displayed by: # Print a summary of the test print(our_test) ## ## One Sample t-test ## ## data: age3_lengths ## t = -18.829, df = 47, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 973.0118 ## 95 percent confidence interval: ## 749.2547 792.4536 ## sample estimates: ## mean of x ## 770.8542 The first line of the output gives us the actual data that with which we are working- nothing of interest here other than a quick sanity check until later on in the course. The second line shows the statistics that we are interested in: t is the calculated value of the test statistic for the t-test in this case. The df, or degrees of freedom, is the number of observations in the sample, minus the number of parameters that we are estimating (in this case, just one: the mean). Our p-value is the probability of observing data that are more extreme than what we observed if the null hypothesis is in fact true (i.e. the probability that rejection of the null is inappropriate). Again, because it is smaller than \\(\\alpha\\) we reject the null and accept the alternative hypothesis. Our alteranive hypothesis (HA) was that the sample mean is not equal to population mean. We can specify other alternatives (and therefore nulls) in this and other models in R. Finally, R reports the mean and the 95% confidence interval of age3_lengths. 6.1.1.2 Assumptions Its always important for us to think about the assumptions that we are making when (read before) conducting a statistical test. First, there are implicit assumptions that we make. For example, we assume that the data are representative of what we are trying to measure and were collected in a random manner with respect to other potentially confounding factors in this case. Then, there are explicit assumptions that we make for specific tests. For the one-sample t-test, the assumption that we really care about is: The data are normally distributed The t-test is generally robust to violations of this assumption provided that sample sizes are large enough (Google Central Limit Theorem, this is The Worst Stats Text eveR). But, it is always good to check. In particular, when we are working with small sample sizes like this example (n = 48), we should really make sure that things look okay or find an alternative tool. 6.1.1.3 Checking assumptions Visual check for normality One simple way to assess our assumption of normality is to look at a plot of the data. As you will see later, we are usually concerned with the residuals, but we can look at the actual data here because we have only one group and if its normal so are its errors. Have a quick look at these to see what we are working with using the histogram code from Chapter 4. I set the x-axis limits below using the maximum Length from the grasscarp data so we can see what part of the length range weve sampled here. ggplot() + geom_histogram(aes(age3_lengths), bins = 30) + scale_x_continuous(limits=c(0, max(grasscarp$Length)), expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + xlab(&quot;Total length (mm)&quot;) + ylab(&quot;Count&quot;) + theme_classic() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) Sure, looks totally normal to me? Wouldnt it be great if there were a statistical test for determining whether this sample is different from the normal? Great that you should ask. 6.1.1.3.1 Tests of normality (Shapiro-Wilk) The Shapiro-Wilk test is commonly used to test normality of a distribution as a check of assumptions. We can use this to test whether our data deviate from normal in the following manner: shapiro.test(age3_lengths) ## ## Shapiro-Wilk normality test ## ## data: age3_lengths ## W = 0.95715, p-value = 0.07746 First, note that the test statistic is the W statistic for this test. Second, we have a p-value of 0.0774637. Oh, no! Wait, what does that mean? For this test, we actually dont want p &lt; 0.05 if we are relying on assumptions of normality, so this is a good thing. But, it doesnt necessarily mean age3_lengths is normally distributed. It just means that we cant tell if the sample we have collected is different from normal (we fail to reject the null but cant accept it). I guess that is good enough, but that p-value is awfully close to 0.05 for my taste. So, are we up that proverbial tributary without a paddle, or can we salvage the mess and move on with life? Dont worry, theres a statistical test for that, too. 6.1.2 Wilcox test You can think of the Wilcox test as a non-parametric analog of the t-test. In general, non-parametric tests tend to be slightly more conservative than the parametric alternatives because they require fewer assumptions. However, non-parametric tests can be useful where our data are not normal, or we dont feel we have sufficient data to say this with confidence (hmmmaybe dont conduct any tests in that case!). For the Wilcox test, we are checking for shifts in the median (not the mean) of one or more samples. Why is this? The mean of a non-normal distribution is not always a useful descriptor of the probability mass under a distribution (it still describes central tendency but does not necessarily describe the place where most of the data are). But, the median always (as in always, always, always) describes central tendency of the data, so we can pretty much use it for describing any sample. This is because the median is defined as the middle value. That is, half of the data should fall on either side of the median if you lined up all of your data on the equator (wrong metaphor?). back to the Wilcox test. # First, do this and have a quick read: ?wilcox.test We can use this test to see if the median length of age-3 fish is statistically different from the median value of length in the sample. wilcox.test( age3_lengths, mu = median(grasscarp$Length), alternative = &#39;less&#39;, # Note different alternative here! exact = FALSE # Not much data, so not exact ) ## ## Wilcoxon signed rank test with continuity correction ## ## data: age3_lengths ## V = 0, p-value = 8.414e-10 ## alternative hypothesis: true location is less than 1007 Interpreting the results is essentially the same as for the t-test, but without the degrees of freedom, so we wont belabor this. Importantly, the test, being robust to any distributional assumptions, should also (and does) tell us that the length of age-3 fish is significantly shorter than the population mean (or median - whichever you used). "],["6.2-two-sample-tests.html", "6.2 Two-sample tests", " 6.2 Two-sample tests Okay, with that out of the way, now we can do some tests that might be a little more meaningful to most people. We can use two-sample tests to determine whether two groups differ in some metric of interest. This lends itself naturally to use in controlled experiments that we conduct in laboratories, for example. 6.2.1 The two-sample t-test If you have been exposed to only one statistical test already it is probably the two-sample t-test. This is a test that is used to test for differences in a continuous dependent variable between two groups. The test statistic itself is pretty trivial to calculate. You can find a video of that here. Seriously, if you have never done a t-test, watch the 6-minute video now. Otherwise, you may not understand what follows. I am not going to go into the math here because this is The Worst Stats Text eveR. The video will also help you understand how ANOVA and other tests work later. Understanding how these tests work will give you phenomenal cosmic powers when it comes to analyzing biological data. If you email asking me how a t-test works, I am going to send you this video. Lets keep working with the grasscarp data for now for the sake of consistency. But, now we want to know if there is a difference in mean length of fish depending on whether their population density is high or low. To look at this, well need to make some groups in our grasscarp data that correspond to years of high and low density. You can compare fish density between years quickly using the summary pipeline demonstrated in Chapter 3 grasscarp %&gt;% group_by(Year) %&gt;% summarize(dens = unique(nha)) ## # A tibble: 5 x 2 ## Year dens ## &lt;int&gt; &lt;int&gt; ## 1 2006 21 ## 2 2007 58 ## 3 2009 44 ## 4 2010 43 ## 5 2017 127 You can see that density was much higher in 2017 than in any of the preceding years. This is because hydrilla area was reduced by several hundred hectares (ha) between 2010 and 2014 (which was actually the reason we went out to collect more data in 2017). But, these are just means and we need to be able to account for the variability in these measurements to call it science. So, lets build some groups based on high and low density years. First, well add a new categorical variable to grasscarp called density, and well fill it all in with the word \"low\" because there is only one year when density was high. grasscarp$density &lt;- &quot;low&quot; Next, well change all of the observations for 2017 to \"high\" so we have low density and high density groupings in our density column. This way, we only have to change the variable for one year. grasscarp$density[grasscarp$Year == 2017] &lt;- &quot;high&quot; Then, well subset the data to look at a single age so our comparisons are fair between years. I picked Age == 10 because 10 years is in the middle of the range of ages in the data set. You can try it with another age as long as there are enough data. mid_carps &lt;- grasscarp %&gt;% subset(Age == 10) Now, we can conduct our two-sample t-test! The syntax is pretty straightforward, and is similar to what we used above, except that now we have two groups so we will omit mu and specify the t-test as a formula with independent (x, density) and dependent (y, Length) variables. There is no pairing of our observations, so we specify paired = FALSE, and we tell R we dont want to assume that the variance of Length is equal between density groups. t.test(Length ~ density, data = mid_carps, paired = FALSE, # 2-sample test, not &quot;paired&quot; var.equal = FALSE, # We make no variance assumption conf.level = 0.95 # Alpha = 0.05 ) ## ## Welch Two Sample t-test ## ## data: Length by density ## t = -3.2263, df = 11.133, p-value = 0.007952 ## alternative hypothesis: true difference in means between group high and group low is not equal to 0 ## 95 percent confidence interval: ## -190.03710 -36.03433 ## sample estimates: ## mean in group high mean in group low ## 987.7143 1100.7500 The interpretation of the results is much the same as with the one-sample t-test, except that we are now testing the null hypothesis that there is no difference between groups. We reject the null hypothesis, and we conclude that age-10 fish were significantly larger during periods of low population density than they were during years of high population density (t = -3.2262903, df = 11.1326183, p &lt; 0.05). Makes perfect sense! 6.2.1.1 Assumptions Equal variance Now that we are using two samples, we should be cognizant that this test assumes equal variances in the independent variable between our two groups. If our variances are not equal, then we need to account for that (R actually assumes that the variances are unequal by default). Lets test to see if the variances were equal between age-10 fish in the high and low density groups. To do this, we will conduct an F-test on the ratio of the two variances. If the ratio of the variances is different than one, we reject the null that the variances are the same. var.test(Length~density, mid_carps) ## ## F test to compare two variances ## ## data: Length by density ## F = 0.72988, num df = 20, denom df = 7, p-value = 0.5423 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.1634026 2.1950440 ## sample estimates: ## ratio of variances ## 0.729877 Wow, this is way to easy. I hope that you are beginning to understand the GLORY OF R. This test could be a real pain in other software programs, and may not even be an option in many. Back on topicwe fail to reject the null hypothesis that the variances were equal. In this case, we now feel validated in the use of a two-sample t-test regardless of what R uses as the default (yes, sarcasm intended). 6.2.1.1.1 Normality Yes, we are still worried about this one because of the reasons given in the previous section. We can check this the same way as before. End of section. 6.2.2 Two-sample Wilcox test If we were in violation of normality, we would use the Wilcox test to test for differences in ranks. I will not go through the whole thing again here. As with the t-test, if you have not been exposed to doing a rank-sum test by hand you really should watch a video of how to do it. It really is easy once youve seen it and the video demystify the test for you. I will note that the syntax is very much the same to that of the t-test now. This will pretty much stay the same for the next 6 chapters. Thank R, not me. wilcox.test(Length~density, mid_carps) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Length by density ## W = 26, p-value = 0.005015 ## alternative hypothesis: true location shift is not equal to 0 As expected, this test also shows that the two samples differ significantly. Note: this is equivelent to the Mann-Whitney U-test you may have learned about elsewhere. Had these samples been paired, R would have defaulted to a signed-rank test, with which you may also be familiar. 6.2.3 Presenting your results While it is important to report the test statistics, df, etc., it can be just as meaningful to give the sample means (reported in the t.test) and show a graph. Remember: dont make shitty graphs. Be proud of your results and show your readers what they mean. In this case, a boxplot or a violin plot would work great. We havent looked at violin plots yet, so lets give them a whirl! Violins are a lot like box plots except they give us a little better visual description of the shape of sampling distributions within groups. I added some ggplot2 functions to control fill and color of the violins in the example below. You can check out this blog post for some other cool examples with other ggplot geometries. Play around with the plotting code above to change what you like. Remember, all of the customization achieved using the theme() function is the same across plot types. Here is a quick, ugly violin plot with some basic options. Pretty easy to make, but also kind of makes you want to puke. ggplot(mid_carps, aes(x = density, y = Length)) + geom_violin(aes(group = density), trim = FALSE) Here is a much better plot. Not that much more difficult to make, and doesnt make you want to puke even if the code does a little bit. mid_carps %&gt;% ggplot(aes(x = density, y = Length, fill = density, color = density)) + geom_violin(aes(group = density), trim = FALSE, size = .75) + scale_x_discrete(breaks=c(&quot;high&quot;, &quot;low&quot;), labels = c(&quot;High&quot;, &quot;Low&quot;)) + scale_fill_grey(start = 0.9, end = 0.4) + scale_color_grey(start = 0.8, end = 0.3) + xlab(&quot;Density&quot;) + ylab(&quot;Total length (mm) at age 10&quot;) + labs(fill = &quot;Density&quot;, color = &quot;Density&quot;) + theme_bw() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) There, isnt that fancy? I think that gives you a much more detailed understanding of how Length varies between high and low population density than a simple p-value. But, maybe its just me "],["6.3-frequency-analysis.html", "6.3 Frequency analysis", " 6.3 Frequency analysis Now, what if we didnt collect very good data, or we binned our data into low-resolution categories for the sake of ease in our study design? Often, and for a variety of reasons other than crappy data collection, we want to compare frequencies of events between two (or more) groups. We may even design studies specifically to test these kinds of hypotheses when we think about rates, for example. This is very common in studies of population genetics [definitely citations available for that one - go Google them] The simplest way to test for differences in the frequency of a categorical response between two groups is (some would argue) the \\(\\chi^2\\) test. The \\(\\chi^2\\) is another one of those that you should really work out by hand because it is used in a variety of settings under the hood of more complex routines. Here is your token video link showing an example. Watch it. Please. 6.3.1 Worked example Lets say we want to know if the number of grass carp in a given age group (say age 10) varies between years. These fish are sterile hybrids, so we would expect that the number of fish in each age would change drastically with increasing time since the year of initial stocking (1995). First, make a table showing the number of fish in each Age by Year with the grasscarp data. agexyear &lt;- with(grasscarp, table(Age, Year)) print(agexyear) ## Year ## Age 2006 2007 2009 2010 2017 ## 1 0 0 0 2 0 ## 2 6 7 7 8 0 ## 3 11 10 10 14 3 ## 4 4 3 6 3 3 ## 5 0 1 3 2 10 ## 6 0 1 2 1 12 ## 7 0 0 1 3 11 ## 8 0 1 1 2 12 ## 9 1 0 1 3 21 ## 10 4 1 1 2 21 ## 11 4 9 0 6 15 ## 12 3 15 2 3 18 ## 13 0 1 4 3 7 ## 14 0 0 11 8 6 ## 15 0 0 2 12 12 ## 16 0 0 0 2 10 ## 17 0 0 0 0 8 ## 18 0 0 0 0 9 ## 19 0 0 0 0 12 ## 20 0 0 0 0 12 ## 21 0 0 0 0 5 ## 22 0 0 0 0 3 ## 23 0 0 0 0 2 Basically what we are going to do is analyze the proportion of total fish in each column by age. You should see some pretty obvious patterns here. We have a couple of things to think about now. First, this is the kind of question you dont need statistics for. Second, we have a whole bunch of empty groups, and these are not random with respect to year. Some of these come from ages that were not yet available in years 2006 - 2010 and some from patterns in fish stocking. The large number of empty pairings and the fact that most age classes had fewer than five fish in any year prior to 2017 means we should probably break the data down a little further. This stinks because we lose resolution, but that is the cost. For the sake of demonstration, lets summarize the data by high and low density again and well look at the number of fish collected in each age class during high and low density years. freqs &lt;- grasscarp %&gt;% # Pass grass carp data frame to group_by() filter(Age %in% c(10:15)) # Select only shared age range head(freqs) ## Year Length Weight_kg Age cohort n_stocked n acre ha nha kg kg_ha ## 1 2006 1262 23.154 12 1994 7000 25128 2957 1203 21 129929 108 ## 2 2006 1138 23.923 10 1996 7000 25128 2957 1203 21 129929 108 ## 3 2006 1187 18.614 10 1996 7000 25128 2957 1203 21 129929 108 ## 4 2006 1207 19.522 12 1994 7000 25128 2957 1203 21 129929 108 ## 5 2006 1086 21.285 10 1996 7000 25128 2957 1203 21 129929 108 ## 6 2006 1095 18.160 12 1994 7000 25128 2957 1203 21 129929 108 ## density ## 1 low ## 2 low ## 3 low ## 4 low ## 5 low ## 6 low We will test the null hypothesis that there is no difference in the number of age-10 fish between high and low densities. # Run the test chi_test &lt;- with(freqs, chisq.test(x = density, y = Age)) # Have a look print(chi_test) ## ## Pearson&#39;s Chi-squared test ## ## data: density and Age ## X-squared = 13.107, df = 5, p-value = 0.0224 And, bam! We see that there is a difference in the frequency of fish collected in each age class in high and low density years. Shocker. Data visualization techniques for contingency table analyses like this seem to have generally lagged behind theory in terms of wide-spread implementation. There is a base R mosaicplot that plots relative freqencies. You can interpret the width of the bars as the proportion of total observations in each age class. Likewise, the height of the vertical segnmens corresponds to proportion of high or low density observations in each Age. mosaicplot(Age ~ density, data = freqs) The need for improved graphical representation for these data types is recognized. There have been recent efforts to extend the philosophies used ggplot() to contingency analysis by r developers (see Wickham and Hofmann 2011). It was even the topic of a recent masters thesis (see Grant 2017). But as far as I know the ideas from these works have not been integrated into ggplot2 or the tidyverse yet. Sorry about the citations. I dont know what I was thinking. This is supposed to be the Worst Stats Text eveR. "],["6.4-next6.html", "6.4 Next steps", " 6.4 Next steps In this chapter, we introduced inferential statistics and walked through examples of a few simple statistical tests for comparing samples to one another using two-sample tests, or to a single value using a one-sample test. In Chapter 7 we will continue to build on these tools as we press on to linear models and the rest of statistics. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
