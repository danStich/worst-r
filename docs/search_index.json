[
["10-Chapter10.html", "10 Communicating effect sizes", " 10 Communicating effect sizes This is the graph showing how the amount of time spent in rehab by sea turtles changed with fishing hook size during three years. We made this in Chapter 3, but I never told you if the effect of hook width was significant because this is The Worst Stats Text eveR. It was not. But then again, you probably could have figured that out from the graph. That’s why it’s important to show your predictions. Here’s the bad news: add/drop is over and we’re about to do some math. Here’s the good news: the math will stay the same from now until the end of this book because it is the beautiful, unifying math behind most of the tools we’ve discussed so far and all the tools to come. Well, I don’t know if that’s actually good news, but it does sound nice when I say it like that. Now that we have a handle on interpreting the statistical results of linear models we need to think about how to communicate biological differences (effects) and the uncertainty associated with our predictions. This is a major short coming of many scientific studies, and has led to wide-spread reporting of statistically significant results that confer minimal biological meaning. On the other hand, if we do have really cool biological results, we want to be able to show those to people! A well designed graphic will tell most of your readers more than a parentheses-packed, numerically dense Results section - I don’t care how well you write. How we approach communication of our results can range from summarizing and graphing raw data to plotting futuristic curves over raw data depending on the type of effect we are trying to communicate. That depends, of course, on the model that we fit, the data that we collected, and how they were collected. To do this well, we have to at least understand how R is using our data, and that requires at least a superficial understanding of the actual math we are doing. Sorry. We’ll take it one step at a time and work through ANOVA (so hard), linear regressions (super easy), and ANCOVA (not that hard once you “get it”). For this chapter, we will revisit some of the built-in data sets we’ve been using for hypothesis testing and linear models and introduce the dental data set. We’ll also be working with a few packages in the tidyverse so you can go ahead and load that now if you want to. library(tidyverse) "],
["10-1-one-way-anova.html", "10.1 One-way ANOVA", " 10.1 One-way ANOVA When we are working in the world of one-way ANOVA, or even more complex models that contain only “main effects” of categorical, explanatory variables, the interpretation of these effects is relatively straightforward. Let’s use the PlantGrowth data as an example. data(&quot;PlantGrowth&quot;) We’ll start here by fitting a one-way anova to test effects of treatment group on on plant weight. # Get the names of the df names(PlantGrowth) ## [1] &quot;weight&quot; &quot;group&quot; # Fit the model mod &lt;- lm(weight~group, data = PlantGrowth) We’ve seen these data and this model before. We know there was a significant effect of treatment on plant weight but only trt1 and trt2 were different when we checked with the Tukey test. So, for now we will ignore the ANOVA table and just look at the summary. summary(mod) ## ## Call: ## lm(formula = weight ~ group, data = PlantGrowth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.0320 0.1971 25.527 &lt;2e-16 *** ## grouptrt1 -0.3710 0.2788 -1.331 0.1944 ## grouptrt2 0.4940 0.2788 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 This summary gives us three coefficients corresponding to the coefficients of a linear model. Up until now, we’ve mostly ignored these for ANOVA and focused on hypothesis testing. But we need to use the coefficients to make predictions from our model and communicate biological results - which is why there is a history of people not doing this effectively. If we wanted to write out that linear model, we could write it like this: \\(y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\) But this is confusing because we only gave R one variable for X! How did it get three? Plus, we’ve been thinking of ANOVA like t-tests. Much puzzlement. 10.1.1 Unifying the linear model To really get full control over making predictions from linear models and the models to come we need to understand a little bit more about what R is doing here. I mentioned in Chapter 9.3 that we would need to start thinking about the linear model as \\(y = \\beta X + \\epsilon\\) or \\(\\sum_{k=1}^{K} \\beta_k X_k + \\epsilon\\) to unify the t-test, ANOVA, linear regression, and ANCOVA into a single general framework. The reason for this is that R (and the math) actually don’t see X in our linear models the way we’ve been writing it in our code. The models we’ve talked about so far are solved through Least Squares estimation. This involves solving for however many \\(\\beta\\) we might have using linear algebra and a little calculus to minimize the sum of \\(\\epsilon^2\\), or our squared residuals. To do the math, X must be a matrix of values that can be multiplied by a vector coefficients (\\(\\beta\\)) because as we now know, \\(y = \\beta X + \\epsilon\\). So, how does this relate to \\(\\beta_0\\) and the fact that we supposedly have three X variables in the PlantGrowth ANOVA even though it is just one column? I’ve already told students in my class by this point in the semester, but I’ll repeat here that \\(\\beta_0\\) has a special place in my heart. It is the thing that allows us to relate all of this crap back to \\(y = mx + b\\) and makes me feel like I understand statistics a little bit. But, it is also the hard part behind understanding the predictions you make from linear models if you don’t know or like (love) the algebra. Especially for ANVOA and ANCOVA-like models. And let’s face it, most of us as biologists don’t understand let alone love the algebra. We’ll try to keep avoiding that here as long as we can. Up until now, we have thought of \\(\\beta_0\\) as our (Intercept) term in linear models, and that is both truthful and useful. But, it is just another \\(\\beta\\) in the matrix multiplication used to solve least-squares regression. How, then, is the intercept represented mathematically in ANOVA? 10.1.2 The model matrix In order to understand how the intercept works in ANOVA, we must look at the model matrix. The model matrix or design matrix is X from the really gross equations that I started showing all of a sudden now that the Add/Drop period has ended. (Muahaha). It really isn’t as sinister as it sounds though. For our plant model, we wrote weight ~ group in our call to lm() and didn’t have to think twice about what was happening after that. In the meantime, R had re-written the equation as \\(y = \\beta_i X_i\\) or y = (Intercept)*model.matrix$(Intercept)+ grouptrt1*model.matix$grouptrt1 + grouptrt2*model.matix$grouptrt2. To begin understanding that difference, we obviously need to see this model.matrix object. First, look at the actual data used to fit the model: head(mod$model) ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl You can see that we have one column for our response, weight, and one column for our explanatory variable, group, just as you thought. Now here is the design matrix: # Extract design matrix from fitted # PlantGrowth model X &lt;- model.matrix(mod) # Have a look head(X) ## (Intercept) grouptrt1 grouptrt2 ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 Okay, that’s actually not so bad. So, this is how R sees our data now. What R has done is to dummy code our group variable from the PlantGrowth data for each row of the data. The first column, (Intercept) contains only 1. You can think of this as X_0 in our linear model. It is multiplied by \\(\\beta_0\\) in \\(y = \\beta_0 + \\beta_k X_k\\). But, since it is always 1 we just don’t write it when we write the formula for a line and \\(\\beta_0\\) is always in the model! OMG that is sooooo annoying. The second column is an indicator variable for whether group is equal to trt1 for a given observation (row in PlantGrowth). If group == trt1 for that row, then the column grouptrt1 gets a 1. If not, it gets a 0. Same for grouptrt2. The columns grouptrt1 and grouptrt2 are each multiplied by their own \\(\\beta\\) in our formula: \\(y = \\beta_{(Intercept)} X_{(Intercept)} + \\beta_{grouptrt1} X_{grouptrt1} + \\beta_{grouptrt1} X_{grouptrt1}\\) If the columns grouptrt1 or grouptrt2 have 0, then \\(\\beta_i X_i = 0\\) and the term for that group falls out of the equation, leaving only ctrl or the (Intercept). We can use this to make predictions directly from our model coefficients. Before moving on to prediction, it is helpful if you think of the coefficients for ANOVA as being an intercept (mean of the alphabetically first group) and offsets, or adjustments, to that intercept for each subsequent group. That is, ANOVA is kind of like a linear model with multiple intercepts and no slopes. We are just estimating a bunch of points on the y-axis. 10.1.3 Prediction Now that we’ve seen what R is actually doing, it becomes pretty trivial to make predictions from one-way ANOVA by hand. We can get the model coefficients (\\(\\beta\\)) like this: # We can get the model coefficients like this: names(mod) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;contrasts&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; ## [13] &quot;model&quot; coeffs &lt;- data.frame(mod$coefficients) # And now we have a vector of beta betas &lt;- coeffs$mod.coefficients We can use betas to make predictions from the formula of our linear model for each group by taking advantage of the dummy coding that R uses. # From the model, we can estimate: # Mean of control y_control &lt;- betas[1] + betas[2]*0 + betas[3]*0 # Mean of trt1 y_trt1 &lt;- betas[1] + betas[2]*1 + betas[3]*0 # Mean of trt2 y_trt2 &lt;- betas[1] + betas[2]*0 + betas[3]*1 Or if you wanted to get really fancy, you could do this with matrix math: # Get unique groups in dummy coded matrix X_pred &lt;- as.matrix(unique(model.matrix(mod))) # Multiply betas by dummy coded # matrix using transpose of both # These are your predictions # for ctrl, trt1, and trt2 y_pred &lt;- t(betas) %*% t(X_pred) Of course, either of these approaches is super useful but R also has default predict() methods for most or all of the models we will work with in this book. We will use these for the most part, as will ggplot(), which is more convenient than you will ever be able to appreciate. To make predictions of y from the original data that you used to fit the model (mod), you can just do this: # Get unique values of groups and put it in # a data frame. The predict function expects # original x variable as a vector or a named # column in a data.frame groups &lt;- data.frame(group = unique(PlantGrowth$group) ) # Make the prediction y &lt;- predict(mod, newdata = groups, interval = &quot;confidence&quot;) # Add it to the data frame pred_plant &lt;- data.frame(groups, y) If we want confidence intervals for the predictions, we can add that, too: # Make the prediction with confidence yCI &lt;- predict(mod, newdata = groups, interval = &quot;confidence&quot;) # Add it to the data frame pred_plantCI &lt;- data.frame(groups, yCI) You could print this and get a nice clean table of estimated means and 95% confidence intervals for each group. print(pred_plantCI) ## group fit lwr upr ## 1 ctrl 5.032 4.627526 5.436474 ## 2 trt1 4.661 4.256526 5.065474 ## 3 trt2 5.526 5.121526 5.930474 Now, let’s compare our model predictions to the actual means. # Calculate group means means &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarize(mean(weight)) print(means) ## # A tibble: 3 x 2 ## group `mean(weight)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 ctrl 5.03 ## 2 trt1 4.66 ## 3 trt2 5.53 Pretty much spot on! 10.1.4 Plotting We could use any number of graphical tools to represent these results. Given that we’ve met the assumptions of normality, and we’ve determined that statistical differences exist, the simplest (and most common) method for visualizing these results is to just show a box plot or a violin plot, or both, with the raw data. Hmmm…I never realized how small this data set was. ggplot(PlantGrowth, aes(x = group, y = weight)) + geom_violin(aes(fill=group), alpha=0.2) + geom_boxplot(aes(fill=group), width = 0.2, alpha = 0.5) + geom_jitter(aes(color=group), width = 0.15, alpha=0.5) + scale_fill_manual(values=c(&#39;black&#39;, &#39;gray30&#39;, &#39;gray60&#39;)) + scale_color_manual(values=c(&#39;black&#39;, &#39;gray30&#39;, &#39;gray60&#39;)) + xlab(&#39;Group&#39;) + ylab(&#39;Weight (g)&#39;) + theme_bw() + theme(axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) This plot is really cool, but it doesn’t actually show us how our model predictions compare to the raw data! However, we could also think of our model predictions as just being different y-intercepts, which will be helpful when we start to work with ANCOVA. If we plotted them that way, they would look like this: ggplot(pred_plantCI, aes(x = 0, y = fit, color = group)) + geom_point(size = 3) + scale_x_continuous(limits = c(-1, 1), expand=c(0,0)) + geom_segment(aes(x = 0, xend = 0, y = lwr, yend = upr), lwd = 1.5, alpha = 0.25) + xlab(&quot;X&quot;) + ylab(&quot;Weight (g)&quot;) + labs(color = &quot;Predicted&quot;) But this is really hard to see and understand. So, we usually look at it like this in keeping with the dummy coding that is used in the model matrix: ggplot(pred_plantCI, aes(x = 1:3, y = fit, color = group)) + geom_point(size = 3) + scale_x_continuous(limits = c(0, 4), expand=c(0, 0)) + geom_segment(aes(x = 1:3, xend = 1:3, y = lwr, yend = upr), lwd = 1.5, alpha = 0.25) + xlab(&quot;X[, i]&quot;) + ylab(&quot;Weight (g)&quot;) + labs(color = &quot;Predicted&quot;) Or, perhaps more mercifully: ggplot(pred_plantCI, aes(x = group, y = fit, color = group)) + geom_point(size = 3) + geom_segment(aes(x = group, xend = group, y = lwr, yend = upr), lwd = 1.5, alpha = 0.25) + xlab(&quot;Treatment group&quot;) + ylab(&quot;Weight (g)&quot;) + labs(color = &quot;Predicted&quot;) Finally, we could put this right over the top of our raw data and/or violin to see how well the model predictions match up with the data: ggplot(PlantGrowth, aes(x = group, y = weight, color = group)) + geom_violin(aes(fill = group), alpha = 0.05) + geom_jitter(size = 1.5, width = 0.05) + geom_point(mapping = aes(x = group, y = fit), data = pred_plantCI, size = 4) + geom_segment(aes(x = group, xend = group, y = lwr, yend = upr), data = pred_plantCI, lwd = 1.5, alpha = 0.5) + theme_bw() + theme(panel.grid = element_blank()) + xlab(&quot;Treatment group&quot;) + ylab(&quot;Weight (g)&quot;) + labs(fill = &quot;Group&quot;, color = &quot;Group&quot;) "],
["10-2-two-way-anova.html", "10.2 Two-way ANOVA", " 10.2 Two-way ANOVA Two-way ANOVA works the same way as one-way ANOVA, except that now we have multiple dummy-coded variables tied up in the intercept. For this example, we will consider a new data set. These data are from an experiment in Restorative Dentistry and Endodontics that was published in 2014. The study examines effects of drying light and resin type on the strength of a bonding resin for teeth. The full citation for the paper is: Kim, H-Y. 2014. Statistical notes for clinical researchers: Two-way analysis of variance (ANOVA)-exploring possible interaction between factors. Restorative Dentistry and Endodontics 39(2):143-147. Here are the data: dental &lt;- read.csv(&#39;data/dental.csv&#39;, stringsAsFactors = FALSE) 10.2.1 Main effects model We will start by fitting a linear model to the data that tests effects of lights and resin on adhesive strength mpa. Since both lights and resin are categorical, this is a two-way ANOVA. We use the + to imply additive, or main-effects only. # We are looking only at main effects for now dental.mod &lt;- lm(mpa ~ lights + resin, data = dental) If we make an ANOVA table for this two-way ANOVA, we see that there are significant main effects of resin type but not lights used for drying. anova(dental.mod) ## Analysis of Variance Table ## ## Response: mpa ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## lights 1 34.7 34.72 0.6797 0.4123 ## resin 3 1999.7 666.57 13.0514 6.036e-07 *** ## Residuals 75 3830.5 51.07 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can also examine the model coefficients for a closer look at what this means. summary(dental.mod) ## ## Call: ## lm(formula = mpa ~ lights + resin, data = dental) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.1162 -4.9531 0.1188 4.4613 14.4663 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.074 1.787 10.676 &lt; 2e-16 *** ## lightsLED -1.318 1.598 -0.824 0.41229 ## resinB 3.815 2.260 1.688 0.09555 . ## resinC 6.740 2.260 2.982 0.00386 ** ## resinD 13.660 2.260 6.044 5.39e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.147 on 75 degrees of freedom ## Multiple R-squared: 0.3469, Adjusted R-squared: 0.312 ## F-statistic: 9.958 on 4 and 75 DF, p-value: 1.616e-06 Remember, in our data we had 2 kinds of lights, and 4 kinds of resin. But, here we have one less of each! Why is this? It is because of the way categorical variables are dummy coded for linear models. But, now we have two separate sets of adjustments, so one level of each variable is wrapped up in the estimate for our intercept (lightsHalogen and resinA). When in doubt, have a look at the model matrix: X &lt;- model.matrix(dental.mod) head(X) ## (Intercept) lightsLED resinB resinC resinD ## 1 1 0 0 0 0 ## 2 1 0 0 0 0 ## 3 1 0 0 0 0 ## 4 1 0 0 0 0 ## 5 1 0 0 0 0 ## 6 1 0 0 0 0 Right now, you might be a little confused about how to calculate and show the effect size for these variables. If you are not, you should probably take a more advanced stats class and get a better book. One reasonable option might be to summarize the data by the means and plot the means, but we already decided that we are going to plot our model predictions against the raw data following the form of the statistical model. Rather than go throught the math again, let’s just use the built-in predict() function that I know you now appreciate! First, we need to do a little magic to get the group combinations for lights and resin into a data.frame that we can use for prediction. groups &lt;- data.frame( with(dental, unique(cbind(lights, resin))) ) Now we can make our predictions: dental_y_pred &lt;- predict( dental.mod, newdata = groups, interval = &quot;confidence&quot; ) pred_dental &lt;- data.frame(groups, dental_y_pred) And now we can plot it just like we did for ANOVA: ggplot(dental, aes(x = lights, y = mpa, color = lights)) + geom_violin(aes(fill=lights), alpha = 0.1) + geom_jitter(size = 1, width = 0.05) + geom_point(mapping = aes(x = lights, y = fit), data = pred_dental, color = &#39;black&#39;, size = 2) + geom_segment( aes(x = lights, xend = lights, y = lwr, yend = upr), data = pred_dental, color = &#39;black&#39;) + facet_wrap(~resin) + theme_bw() + theme(panel.grid = element_blank()) + xlab(&quot;Treatment group&quot;) + ylab(&quot;Weight (g)&quot;) + labs(fill = &quot;Group&quot;, color = &quot;Group&quot;) Now, this looks really nice, but there is definitely something funky going on with Halogen in panel D in the figure above! We have clearly done a poor job of predicting this group. The reason for this, in this case, is because we need to include an interaction term in our model, which makes things even grosser in terms of the math, but is easy to do in R. Of course, if we had been doing a good job of data exploration and residual analysis, we would have noticed this before making predictive plots… 10.2.2 Interactions To make a model that includes an interaction between lights and resin in the dental data, we will need to go all the way back to our model fitting process. # The &quot;*&quot; operator is shorthand for what we want to do # here - more of an &quot;advanced&quot; stats topic dental_int &lt;- lm(mpa ~ lights * resin, data = dental) We just have three more columns in our model matrix to distinguish between coefficients for resin that correspond to LED and coefficients for resin that correspond to Halogen. It is at this point that not even I want to do the math by hand! # Have a look on your own: head(model.matrix(dental_int)) ## (Intercept) lightsLED resinB resinC resinD lightsLED:resinB lightsLED:resinC ## 1 1 0 0 0 0 0 0 ## 2 1 0 0 0 0 0 0 ## 3 1 0 0 0 0 0 0 ## 4 1 0 0 0 0 0 0 ## 5 1 0 0 0 0 0 0 ## 6 1 0 0 0 0 0 0 ## lightsLED:resinD ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 The process for making predictions, thankfully, is identical to two-way ANOVA in R. Using the groups we made for the main-effects model: int_y_pred &lt;- predict( dental_int, newdata = groups, interval = &quot;confidence&quot; ) int_pred &lt;- data.frame(groups, int_y_pred) And now we plot the predictions against the raw data changing only the name of the data containing our predictions, int_pred. ggplot(dental, aes(x = lights, y = mpa, color = lights)) + geom_violin(aes(fill=lights), alpha = 0.1) + geom_jitter(size = 1, width = 0.05) + geom_point(mapping = aes(x = lights, y = fit), data = int_pred, color = &#39;black&#39;, size = 2) + geom_segment( aes(x = lights, xend = lights, y = lwr, yend = upr), data = int_pred, color = &#39;black&#39;) + facet_wrap(~resin) + theme_bw() + theme(panel.grid = element_blank()) + xlab(&quot;Treatment group&quot;) + ylab(&quot;Weight (g)&quot;) + labs(fill = &quot;Group&quot;, color = &quot;Group&quot;) You should see below that all of our means match up much better with the observed data in the violins. And, if you go back to the ANOVA output for this model you will see that the interaction term is significant even though lights still is not on it’s own. In coming chapters, we’ll talk about how to design and compare multiple models like these to compare meaningful biological hypotheses against one another. anova(dental_int) ## Analysis of Variance Table ## ## Response: mpa ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## lights 1 34.72 34.72 1.1067 0.2963 ## resin 3 1999.72 666.57 21.2499 5.792e-10 *** ## lights:resin 3 1571.96 523.99 16.7043 2.457e-08 *** ## Residuals 72 2258.52 31.37 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Interactions between categorical variables generally are more complicated to deal with than interactions between categorical and continuous variables, because then we are only dealing with straight lines that differ by level. This does not, however, make it any less important for us to communicate how our models fit the observations we have collected. If you can get these tools under your belt, they will be extremely powerful for preparing journal articles, and perhaps more importantly, for communicating your results and the uncertainty surrounding them to stakeholders and public audiences. "],
["10-3-linear-regression.html", "10.3 Linear regression", " 10.3 Linear regression Compared to interpreting group effects from ANOVA, the interpretation of a single, continuous predictor in linear regression is pretty straightforward. Here, all we are doing is looking to use the equation for a line \\(y = mx + b\\) to predict the effects of one continuous variable on another. Most of us probably did this for the first time in middle school. But, if we look at the math in the same way that we did for ANOVA, it will make understanding ANCOVA a lot easier. Let’s use the swiss data again for this like we did in Chapter 8. Remember that this data set compares fertility rates to a number of socio-economic indicators: data(&quot;swiss&quot;) We’ll make a model to predict the effect of education level on fertility: # Make the model and save it to a named object called &#39;swiss.mod&#39; swiss.mod &lt;- lm(Fertility ~ Education, data = swiss) Have a look at the design matrix and you can see that R still includes a column for the intercept that is all 1, so this is the same as ANOVA. But, instead of having dummy variables in columns representing groups, we just have our observed values of Education head( model.matrix(swiss.mod) ) ## (Intercept) Education ## Courtelary 1 12 ## Delemont 1 9 ## Franches-Mnt 1 5 ## Moutier 1 7 ## Neuveville 1 15 ## Porrentruy 1 7 Next, we can look at the coefficient estimates for swiss.mod. Remember that each of these coefficients corresponds to one and only one column in our design matrix X. # Summarize the model summary(swiss.mod) ## ## Call: ## lm(formula = Fertility ~ Education, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.036 -6.711 -1.011 9.526 19.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79.6101 2.1041 37.836 &lt; 2e-16 *** ## Education -0.8624 0.1448 -5.954 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.446 on 45 degrees of freedom ## Multiple R-squared: 0.4406, Adjusted R-squared: 0.4282 ## F-statistic: 35.45 on 1 and 45 DF, p-value: 3.659e-07 10.3.1 Prediction As with the case of categorical explanatory variables, we are now interested in predicting the mean expected Fertility for any given value of Education based on our model coefficients. Recall from ANOVA that we can do this “by hand”: betas &lt;- swiss.mod$coefficients X_pred &lt;- as.matrix(model.matrix(swiss.mod)) # Multiply betas by dummy coded # matrix using transpose of both # These are your predictions # for ctrl, trt1, and trt2 y_pred &lt;- as.vector( t(betas) %*% t(X_pred) ) swiss_pred &lt;- data.frame(swiss, y_pred) Or, we can use the built-in predict() function to get confidence intervals on our predictions, too! That’s a pain to do by hand every time, so we will use the predict() function from here out for linear regression! Here I’ll ask R for \"prediction\" intervals. To avoid warnings about predicting from the same data to which the model was fit, we need to either pass the model part of swiss.mod to the function as new data or we need to simulate new data. As models become increasingly complex, it becomes increasingly complicated to simulate data appropriately. Therefore, if I am just interested in communicating my results, I do so with the model data. # Make predictions using the model data y_pred2 &lt;- predict(swiss.mod, newdata = swiss.mod$model, interval = &quot;prediction&quot;) # Combine with original data swiss_pred2 &lt;- data.frame(swiss, y_pred2) Whichever way you do this, you’ll notice that we have a unique value of fit for every value of Education in the original data because fit is predicted as a continuous function of Education in this case: head( swiss_pred2) ## Fertility Agriculture Examination Education Catholic ## Courtelary 80.2 17.0 15 12 9.96 ## Delemont 83.1 45.1 6 9 84.84 ## Franches-Mnt 92.5 39.7 5 5 93.40 ## Moutier 85.8 36.5 12 7 33.77 ## Neuveville 76.9 43.5 17 15 5.16 ## Porrentruy 76.1 35.3 9 7 90.57 ## Infant.Mortality fit lwr upr ## Courtelary 22.2 69.26186 50.03294 88.49077 ## Delemont 22.2 71.84891 52.61363 91.08418 ## Franches-Mnt 20.2 75.29831 55.99275 94.60387 ## Moutier 20.3 73.57361 54.31199 92.83522 ## Neuveville 20.6 66.67480 47.41244 85.93717 ## Porrentruy 26.6 73.57361 54.31199 92.83522 We could also make predictions for specific values of Education by creating (simulating) new values of Education. Below, we make a sequence of new values for Education from the minimum to the maximum in 30 equal increments and then make predictions with that object instead of the model data. new_ed &lt;- data.frame( Education = seq(from = min(swiss$Education), to = max(swiss$Education), length.out = 30 ) ) Now make predictions across the range of observed data. new_y_preds &lt;- predict(swiss.mod, newdata = new_ed, interval = &quot;prediction&quot;) new_preds &lt;- data.frame(new_ed, new_y_preds) Or, you could make a prediction for a single value. Let’s say I ask you to find the mean and 95% confidence interval for a specific value of Education. Usually we are interested in the maximum and minimum for communicating change in y across the range of x. To do this, you can just make some new data and print the predictions! # Make a data frame containing only the max and min values for Education point_ed = data.frame(Education = c(min(swiss$Education), max(swiss$Education))) # Predict new values of y from the model point_y_pred &lt;- predict(swiss.mod, point_ed, interval = &#39;confidence&#39;) # Put the predictions in a data frame with # the min and max values of Education point_preds &lt;- data.frame(point_ed, point_y_pred) # Now you can see your predictions print(point_preds) ## Education fit lwr upr ## 1 1 78.74771 74.72578 82.76963 ## 2 53 33.90549 21.33635 46.47464 Now, it is really easy for us to say: Fertility rate was inversely related to Education (t = 5.95m, p &lt; 0.05), and Education explained about 44% of the variation in Fertility rate. Across the range of observed education values Fertility decreased from a maximum of 78 (95% CI 75 - 83) at Education of 1 to a minimum of 34 (95% CI 21 - 46) at Education of 53 (Figure 1). Where is Figure 1? 10.3.2 Plotting Once we are happy with our predictions, we can go ahead and plot them against the raw data to show how our model fit the data. Here is the code that we used to do this in Chapter 7 but a little more purpley. # Make a pretty plot showing raw data and model predictions ggplot(swiss_pred2, aes(x = Education, y = Fertility)) + geom_point(colour = &#39;plum3&#39;, fill = &#39;plum3&#39;, alpha = 0.75, size = 4) + geom_line( aes(y = fit), size = 1, color=&#39;purple&#39;, alpha = .5) + geom_ribbon(aes(ymin = lwr, ymax = upr), color = &#39;purple4&#39;, fill = &#39;lavender&#39;, alpha = .4, lty = 2, lwd = .5) + theme_bw() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Now that is a money figure that shows your raw data, the model predictions, and the uncertainty associated with both of these. That’s what we want to go for every time - with or without the purpleyness. Multiple regression proceeds in much the same way. In most cases, it is easiest to make model predictions directly from the observed data because when we have multiple continuous X variables they are often correlated with one another. We will examine this in detail in Chapter 11 when we discuss model selection. "],
["10-4-ancova.html", "10.4 ANCOVA", " 10.4 ANCOVA Now, we are going to step up the complexity a little bit and start to look at how to interpret linear models with more than one variable, and more than one variable type. Exciting, I know! Last week we worked with the crickets data to demonstrate ANCOVA. Let’s keep working with that one. # Read cricket data # This data set contains pulses of # 2 species of crickets collected under # varying temperatures crickets &lt;- read.csv(&#39;data/crickets.txt&#39;) We investigated the additive effects of Species and temperature (Temp) on chirpy pulses of individual crickets and found significant evidence of both. # Fit the model cricket.mod &lt;- lm(Pulse~Species + Temp, data=crickets) Here is the summary of the linear model: summary(cricket.mod) ## ## Call: ## lm(formula = Pulse ~ Species + Temp, data = crickets) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0128 -1.1296 -0.3912 0.9650 3.7800 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.21091 2.55094 -2.827 0.00858 ** ## Speciesniv -10.06529 0.73526 -13.689 6.27e-14 *** ## Temp 3.60275 0.09729 37.032 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.786 on 28 degrees of freedom ## Multiple R-squared: 0.9896, Adjusted R-squared: 0.9888 ## F-statistic: 1331 on 2 and 28 DF, p-value: &lt; 2.2e-16 And the model matrix: X &lt;- model.matrix(cricket.mod) You can see that the model matrix still has a column for the (Intercept) that represents Species ex and a dummy variable called Speciesniv to indicate rows in the cricket data where Species == niv. But, now we also have a column in the model matrix for a continuous variable. Not to fear, the math works exactly the same way as it did for ANOVA and for linear regression. 10.4.1 Prediction We could do this using linear algebra (matrix math). Note that the math has stayed the same for ANOVA, regression and ANCOVA. That is because they are all just different special cases of the same general model. X_pred &lt;- as.matrix(model.matrix(cricket.mod)) betas &lt;- cricket.mod$coefficients # Multiply betas by dummy coded # matrix using transpose of both # These are your predictions # for ctrl, trt1, and trt2 y_pred &lt;- as.vector( t(betas) %*% t(X_pred) ) cricket_pred &lt;- data.frame(crickets, y_pred) But since it is a pain to get prediction intervals like this, we’ll use the default predict() function here as well. I am not going to lie, I am literally just copying and pasting code from ANOVA and regression here and changing the names. This is the power of understanding what actually goes on under the hood for us! # Make predictions y_pred &lt;- predict(cricket.mod, interval = &quot;prediction&quot;) # Combine with original data cricket_pred &lt;- data.frame(crickets, y_pred) 10.4.2 Plotting Plot the predictions by species. Again, I am pretty much changing the names of the data and the colors at this point. Who’d have thought that fitting and and making predictions from scary ANCOVA models could be so easy!? Dangerously easy… In this case, though, we should expect to see two lines on our graph if I have not completely lied to you. This is because we have both categorical and continuous explanatory variables in X. Remember that the \\(\\beta\\)s for categorical variables are just adjustments or offsets to the intercept in linear models. That means that we should have two parallel lines given that we had two groups (so one intercept + 1 offset) and a single slope. # Make a pretty plot showing raw data and model predictions ggplot(cricket_pred, aes(x = Temp, y = Pulse, group = Species)) + geom_point(aes(colour = Species, fill = Species), alpha = 0.75, size = 4) + geom_line( aes(y = fit, color = Species), size = 1, alpha = .5) + geom_ribbon(aes(ymin = lwr, ymax = upr, color = Species, fill = Species), alpha = 0.25) + scale_fill_manual(values = c(&#39;gray40&#39;, &#39;black&#39;)) + scale_color_manual(values = c(&#39;gray40&#39;, &#39;black&#39;)) + xlab(expression(paste(&quot;Temperature ( &quot;, degree, &quot;C)&quot;))) + theme_bw() + theme(panel.grid = element_blank()) Ta-da! "],
["10-5-next10.html", "10.5 Next steps", " 10.5 Next steps Here, we have demonstrated how to communicate the biological predictions of statistical models that we use to test hypotheses. These included most common frameworks for data analysis within the context of linear models. We will continue to apply these tools to as we extend our modeling framework to include non-normal response variables of interest in later chapters. First, in Chapter 11, we’ll explore model selection as a way of choosing between hypotheses that are represented by all of these different models. "]
]
