[
["3-Chapter3.html", "3 Working with data", " 3 Working with data American shad, the best fish, lost in the data deluge. Let’s figure out how to make some sense of it. The purpose of this chapter is to get you comfortable working with data in R and give you some tools for summarizing those data in a meaningful way. This is not meant to be a comprehensive treatment of these subjects but rather an introduction to the tools that are available to you (say it with me: “Worst Stats Text eveR”). There are a lot of tools out there and you may come up with something that works better for you once you have some basics under your belt. Now that you have a handle on the types of data you can expect to run into in R, let’s have a look at how we read and work with data that we get from the real world. We will work with the ctr_fish.csv file for Chapter 3, so you will need to download the class data sets that go with this book to play along. "],
["3-1-data-read.html", "3.1 Data read", " 3.1 Data read There are few things that will turn someone away from a statistical software program faster than if they can’t even figure out how to get the program to read in their data. So, we are going to get it out of the way right up front! Let’s start by reading in a data file - this time we use real data. The data are stored in a “comma separated values” file (.csv extension). This is a fairly universal format, so we read it in using the fairly universal read.csv() function. This would change depending on how the data were stored, or how big the data files were, but that is a topic further investigation for a later date. I probably do 95% of my data reads using .csv files. Important Remember that I am assuming your scripts are in the same directory (folder) on your computer as where you downloaded and unzipped the class data (see here for reminder). Before you can read this file you will need to set your working directory. For class, I will ask that you click Session &gt; Set Working Directory &gt; To Source File Location. This will set the working directory to wherever you have saved your code so that R can find the folder data and the files inside of it. # Start by reading in the data am_shad &lt;- read.csv(&quot;data/ctr_fish.csv&quot;) Once you’ve read your data in, it’s always a good idea to look at the first few lines of data to make sure nothing looks ‘fishy’. Haha, I couldn’t help myself. These are sex-specific length and age data for American shad (Alosa sapidissima) from the Connecticut River, USA. The data are used in some models that I maintain with collaborators from NOAA Fisheries, the US Geological Survey, the US Fish and Wildlife Service, and others. The data were provided by CT Department of Energy and Environmental Protection (CTDEEP) and come from adult fish that return to the river from the ocean each year to spawn in fresh water. You can look at the first few rows of data with the head() function: # Look at the first 10 rows head(am_shad, 10) ## Sex Age Length yearCollected backCalculated Mass ## 1 B 1 13 2010 TRUE NA ## 2 B 1 15 2010 TRUE NA ## 3 B 1 15 2010 TRUE NA ## 4 B 1 15 2010 TRUE NA ## 5 B 1 15 2010 TRUE NA ## 6 B 1 15 2010 TRUE NA ## 7 B 1 16 2010 TRUE NA ## 8 B 1 16 2010 TRUE NA ## 9 B 1 16 2010 TRUE NA ## 10 B 1 16 2010 TRUE NA The NA values are supposed to be there. They are missing data. And, don’t forget about your old friend str() for a peek at how R sees your data. This can take care of a lot of potential problems later on. # Look at the structure of the data str(am_shad) ## &#39;data.frame&#39;: 16946 obs. of 6 variables: ## $ Sex : Factor w/ 2 levels &quot;B&quot;,&quot;R&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Age : int 1 1 1 1 1 1 1 1 1 1 ... ## $ Length : num 13 15 15 15 15 15 16 16 16 16 ... ## $ yearCollected : int 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ... ## $ backCalculated: logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ Mass : int NA NA NA NA NA NA NA NA NA NA ... There are about 17,000 observations (rows) of 6 variables (columns) in this data set. Here is a quick breakdown: Sex: fish gender. B stands for ‘buck’ (males), R stands for ‘roe’ (females). Age: an integer describing fish age. Length: fish length at age (cm). yearCollected: the year in which the fish was caught. backCalculated: a logical indicating whether or not the length of the fish was back-calculated from aging. Mass: the mass of indnividual fish (in grams). Note that this is NA for all ages that were estimated from hard structures (so all cases for which backCalculated == TRUE). "],
["3-2-quick-data-summaries.html", "3.2 Quick data summaries", " 3.2 Quick data summaries There are a number of simple ways to summarize data quickly in base R. We already looked at a few of these in previous chapters. But what about something a little more in-depth? One quick way to look at your data is using the summary() function summary(am_shad) ## Sex Age Length yearCollected backCalculated ## B:9512 Min. :1.000 Min. : 3.00 Min. :2010 Mode :logical ## R:7434 1st Qu.:2.000 1st Qu.:31.00 1st Qu.:2011 FALSE:3046 ## Median :3.000 Median :38.00 Median :2012 TRUE :13900 ## Mean :3.155 Mean :36.39 Mean :2012 ## 3rd Qu.:4.000 3rd Qu.:43.00 3rd Qu.:2013 ## Max. :7.000 Max. :55.00 Max. :2014 ## ## Mass ## Min. : 0 ## 1st Qu.: 900 ## Median :1120 ## Mean :1173 ## 3rd Qu.:1440 ## Max. :3280 ## NA&#39;s :14115 This is useful for getting the big-picture. For continuous variables (e.g., Age and Length) R will report some descriptive statistics like the mean, median, and quantiles. For discrete variables (e.g. Sex and backCalculated) we get the mode (if not factor or chr) and counts of observations within each discrete level (e.g. number of observations of B and R in the variable Sex). But, this approach doesn’t really give us much info. We can create more meaningful summaries pretty easily if we install and load some packages like we talked about in Chapter 1, and then look at different ways of sub-setting the data with base R and some methods that might be a little more intuitive for you. "],
["3-3-subsetting-and-selecting-data.html", "3.3 Subsetting and selecting data", " 3.3 Subsetting and selecting data Before we can make meaningful data summaries, we will probably need to re-organize our data in a logical way (through sub-setting or selected specific chunks of data). A lot of times, we do this along the way without really thinking about it. 3.3.1 Manual subsets and selections We talked a little about sub-setting data with logical queries in Chapter 2. Now, let’s refresh and take that a little further to see why we might want to do that. First, we’ll select just the data from am_shad where backCalculated was FALSE. This will give us only the measured Length and Mass for each of the fish, along with their Sex and yearCollected. I’ll call this new object measured. Remember, am_shad is a data frame, so it has two dimensions when we use [ ] for sub-setting and these are separated by a comma, like this: object[rows, columns]. When we leave the columns blank, R knows that it should keep all of the columns. measured &lt;- am_shad[am_shad$backCalculated == FALSE, ] We could do this for as many conceivable conditions in our data on which we may wish to subset data, but the code can get clunky and hard to manage. For example can you imagine re-writing this if you just want to select age six roes without back-calculated lengths? # Notice how we string together multiple # conditions with &quot;&amp;&quot;. If these were &#39;or&#39; # we would use the vertical pipe &quot;|&quot; age_6_rows_measured &lt;- am_shad[am_shad$backCalculated == FALSE &amp; am_shad$Sex == &quot;R&quot; &amp; am_shad$Age == 6, ] 3.3.2 Subsetting and summaries in base R This notation can be really confusing to folks who are just trying to learn a new programming language. Because of that, there are great functions like subset() available that are more intuitive. You could also subset the data using the following code: measured &lt;- subset(am_shad, backCalculated == FALSE) We could also get our age-six females from the previous example using this approach, and at least the code is a little cleaner: age_6_roes_measured &lt;- subset(am_shad, backCalculated == FALSE &amp; Sex == &quot;R&quot; &amp; Age == 6 ) Both do the same thing, but we’ll see below that using subset is preferable if we plan on chaining together a bunch of data manipulation commands using pipes (%&gt;%). Next, we might be interested to know how many fish we have represented in each Sex. We can find this out using the table function in base R: # Here, I use the column name because # we just want all observations of a single # variable. Be careful switching between names, # numbers, and $names! table(measured[&#39;Sex&#39;]) ## ## B R ## 1793 1253 We see that we have 1793 females and 1253 males. We can also get tallies of the number of fish in each Age for each Sex if we would like to see that: table(measured$Sex, measured$Age) ## ## 3 4 5 6 7 ## B 255 848 579 108 3 ## R 0 361 658 220 14 But, what if we wanted to actually calculate some kind of summary statistic, like a mean and report that by group? For our age-6 females example, it would look like this: age_6_roes_measured &lt;- subset(am_shad, backCalculated == FALSE &amp; Sex == &quot;R&quot; &amp; Age == 6 ) age_6_female_mean &lt;- mean(age_6_roes_measured$Length) Again, we could do this manually, but would require a lot of code for a simple calculation if we use the methods above all by themselves to get these for each age group of roes. We would basically just copy-and-paste the code over and over to force R into making the data summaries we need. Nothing wrong with this approach, and it certainly has its uses for simple summaries, but it can be cumbersome and redundant. It also fills your workspace up with tons of objects that are hard to keep track of and that will cause your code-completion suggestions to be wicked annoying in RStudio. That usually means there is a better way to write the code… 3.3.3 Subsetting and summaries in the tidyverse Long ago, when I was still a noOb writing R code with a stand-alone text editor and a console there were not a ton of packages available for the express purpose of cleaning up data manipulation in R. The one I relied on most heavily was the plyr package. Since then, R has grown and a lot of these functions have been gathered under the umbrella of the tidyverse, which is a collection of specific R packages designed to make the whole process less painful. These include packages like dplyr (which replaced plyr) and others that are designed to work together with similar syntax to make data science (for us data manipulation and presentation) a lot cleaner and better standardized. We will rely heavily on packages in the tidyverse throughout this book. Before we can work with these packages, however, we need to install them - something we haven’t talked about yet! Most of the critical R packages are hosted through the Comprehensive R Archive Network, or CRAN. Still, tons of others are available for installation from hosting services like GitHub and GitLab. If you haven’t seen it yet, here is a three-minute video explaining how to install packages using RStudio. It is also easy to do this by running a line of code in the console. We could install each of the packages in the tidyverse separately, but because the packages are so closely related, we can also get all of them at once. Follow the instructions in the link above, or install the package from the command line: install.packages(&#39;tidyverse&#39;) Once we have installed these packages, we can use the functions in them to clean up our data manipulation pipeline and get some really useful information. "],
["3-4-better-data-summaries.html", "3.4 Better data summaries", " 3.4 Better data summaries Now, we’ll look at some slightly more advanced summaries. Start by loading the dplyr package into your R session with library(dplyr) We can use functions from the dplyr package to calculate mean Length of fish for each combindation of Sex and Age group much more easily than we did for a single group above. First, we group the data in measured data frame that we created previously using the group_by function. For this, we just need to give R the data frame and the variables by which we would like to group: g_lengths &lt;- group_by(measured, Sex, Age) This doesn’t change how we see the data much (it gets converted to a tibble), just how R sees it. Next, we summarize the variable Length by Sex and Age using the summarize function: sum_out &lt;- summarize(g_lengths, avg = mean(Length)) head(sum_out) ## # A tibble: 6 x 3 ## # Groups: Sex [2] ## Sex Age avg ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 B 3 38.1 ## 2 B 4 40.5 ## 3 B 5 42.0 ## 4 B 6 43.4 ## 5 B 7 46.8 ## 6 R 4 45.0 Wow! That was super-easy! Finally, to make things even more streamlined, we can chain all of these operations together using the %&gt;% function from magrittr. This really cleans up the code and gives us small chunks of code that are easier to read than the dozens of lines of code it would take to do this manually. # This will do it all at once! sum_out &lt;- # Front-end object assignment measured %&gt;% # Pass measured to the group_by function group_by(Sex, Age) %&gt;% # Group by Sex and age and pass to summarize summarize(avg = mean(Length)) We could also assign the output to a variable at the end, whichever is easier for you to read: measured %&gt;% # Pass measured to the group_by function group_by(Sex, Age) %&gt;% # Group by Sex and age and pass to summarize summarize(avg = mean(Length)) -&gt; sim_out # Back-end object assignment And, it is really easy to get multiple summaries out like this at once: sum_out &lt;- measured %&gt;% group_by(Sex, Age) %&gt;% summarize(avg = mean(Length), s.d. = sd(Length)) head(sum_out) ## # A tibble: 6 x 4 ## # Groups: Sex [2] ## Sex Age avg s.d. ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 3 38.1 2.75 ## 2 B 4 40.5 2.70 ## 3 B 5 42.0 2.29 ## 4 B 6 43.4 2.09 ## 5 B 7 46.8 1.61 ## 6 R 4 45.0 2.65 Isn’t that slick? Just think how long that would have taken most of us in Excel! This is just one example of how functions in packages can make your life easier and your code more efficient. Now that we have the basics under our belts, lets move on to how we create new variables. "],
["3-5-creating-new-variables.html", "3.5 Creating new variables", " 3.5 Creating new variables There are basically two ways to create new variables: we can modify an existing variable (groups or formulas), or we can simulate new values for that variable (random sampling.) If we have a formula that relates two variables, we could predict one based on the other deterministically. For example, I have fit a length-weight regression to explain the relationship between Length and Mass using the am_shad data we’ve worked with in previous sections. This relationship looks like your old friend \\(y = mx + b\\), the equation for a line, but we log10-transform both of the variables before fitting the line (more to come later in the class). Using this relationship, we can predict our independent variable (Mass) from our dependent variable (Length) if we plug in new values for Length and the parameters of the line. In this case, I know that m = 3.0703621, and b = -1.9535405. If I plug these numbers in to the equation above, I can predict log10(Mass) for new lengths log10(Length): \\(log_{10}Mass = 3.0703621 \\cdot log_{10}Length - 1.9535405\\) In R, this looks like: # Parameters from length-weight regression m &lt;- 3.0703621 b &lt;- 1.9535405 # Make a sequence of new lengths based on range in data, # then take the log of the whole thing all at once. log_length &lt;- log10( seq(min(am_shad$Length), max(am_shad$Length), 1) ) # Calculate a new thing (log10_mass) using parameters for line # and sequence of new log10_length. log_mass &lt;- m * log_length + b # Plot the prediction plot(x = log_length, y = log_mass, type = &quot;l&quot;) "],
["3-6-data-simulation.html", "3.6 Data simulation", " 3.6 Data simulation The point of simulation is usually to account for uncertainty in some process (i.e. we could just pick a single value if we knew it). This is almost always done based on probability. There are a number of ways we could do this. One is by drawing from some probability distribution that we have described, and the other is by randomly sampling data that we already have. 3.6.1 Random sub-samples from a dataset Let’s say we want to take random samples from our huge data set so we can fit models to a subset of data and then use the rest of our data for model validation in weeks to come. We have around 17,000 observations in the am_shad data set. But, what if we wanted to know what it would look like if we only had 100 samples from the same population? First, tell R how many samples you want. n_samples &lt;- 100 Now let’s take two samples of 100 fish from our dataframe to see how they compare: # Randomly sample 100 rows of data from our data frame two different # times to see the differences samp1 &lt;- am_shad[sample(nrow(am_shad), size = n_samples, replace = FALSE), ] samp2 &lt;- am_shad[sample(nrow(am_shad), size = n_samples, replace = FALSE), ] # We can look at them with our histograms par(mfrow = c(1, 2)) hist(samp1$Length, main = &quot;&quot;, ylim = c(0, 30)) hist(samp2$Length, main = &quot;&quot;, ylim = c(0, 30)) *If you are struggling to get your plotting window back to “normal” after this, you can either click the broom button in your “Plots” window, or you can run the following code for now: 3.6.2 Stochastic simulation Now, instead of sampling our data let’s say we have some distribution from which we would like sample. So, let’s make a distribution. We will start with the normal, and we can move into others when we talk about probability distributions and sample statistics in Chapter 5. For this, we will use the distribution of American shad lengths for age-6 females because it approximates a normal distribution. We will calculate the mean and sd because those are the parameters of the normal distribution. Start by looking at the size distribution for age 6 females. We use the tidy workflow here with really awful default graphics (more to come in Chapter 4), but we add two arguments to our subset call. We want to select only the variable Length from am_shad, and we want to drop all other information so we can send the output straight to the hist() function as a vector. am_shad %&gt;% subset(Age == 6 &amp; Sex == &quot;R&quot;, select=&#39;Length&#39;, drop=TRUE) %&gt;% hist(main = &quot;&quot;) Now, let’s calculate the mean and sd of Length for age 6 females. We use na.rm = TRUE to tell R that it needs to ignore the NA values. # Calculate the mean Length x_bar &lt;- am_shad %&gt;% subset(Age == 6 &amp; Sex == &quot;R&quot;, select=&#39;Length&#39;, drop=TRUE) %&gt;% mean # Calculate standard deviation of Length sigma &lt;- am_shad %&gt;% subset(Age == 6 &amp; Sex == &quot;R&quot;, select=&#39;Length&#39;, drop=TRUE) %&gt;% sd Note that we could also use the filter() function from the dplyr package for this job, and for big data sets it would be a lot faster for un-grouped data. Now, we can use the mean and standard deviation to randomly sample our normal distribution of lengths. length_sample &lt;- rnorm(n = 10000, mean = x_bar, sd = sigma) # Plot the sample to see if it is a normal- YAY it is! hist(length_sample, col = &quot;gray&quot;, main = &quot;&quot;, xlab = &quot;Forklength (cm)&quot; ) We’ve add a couple of new arguments to the histogram call to make it a little less ugly here. In [Chapter 4] we are going to ramp it up and play with some plots! "],
["3-7-next-steps.html", "3.7 Next steps", " 3.7 Next steps In this chapter, we provided a general overview of our work flow when it comes to reading in data and manipulating them to get useful summaries. In Chapter 4 we will use these processes to help us visualize important trends in these summaries before we begin working with descriptive statistics and sampling distributions in Chapter 5. "]
]
