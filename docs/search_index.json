[["5-Chapter5.html", "5 Sampling distributions in R", " 5 Sampling distributions in R If we can describe the shape of a probability distribution for a random variable like temperature we can make predictions about the world. Sinister? Maybe. These are temperatures that I simulated from the Hudson River using historical data to estimate parameters of a multivariate normal distribution (muwahahaha). In this Chapter, well talk about probability and probability distributions as a backdrop for the models that we will be working with during the next several chapters. When we describe these from data we have collected, we call them sampling distributions. Probability theory is central to statistical techniques, so it will be important for you to have a pretty firm understanding of this to grab hold of big ideas later on. For now, play along and try to understand how they work. Well swing back later for a refresher. In order to complete this Chapter, you will need to have the ggplot2, MASS, and Rlab packages loaded. The only one you should need to install is the Rlab package because MASS is installed when you install R, and we already installed ggplot2 with the tidyverse in Chapter3. Im going to load these now. In general, it is good practice to put these at the top of the script so we know they are needed. library(ggplot2) library(MASS) library(Rlab) None of the class data are required to complete this chapter. "],["5.1-what-are-sampling-distributions.html", "5.1 What are sampling distributions?", " 5.1 What are sampling distributions? When we talk about sampling distributions, we are talking about the probability that a variable we can measure (e.g. temperature) takes on some value. In most cases, there is a higher probability that the variable will take on certain values than others. That probability may be governed by any number of processes and thus may assume a number of different shapes with respect to the likelihood of any given value of our variable. The differences in the shapes that we assume, and the mathematical parameters that we use to describe those shapes are called probability distributions. And, when they are estimated from data, they are sampling distributions. There was a time when biologists were largely restricted to using models that relied heavily on the assumption that the things we measured, and their errors, followed normal distributions, which you have probably heard of or seen in a scientific paper. This was because of how computationally intensive other methods were. This often led to the use of strictly parametric tools like ANOVA and t-tests, or the use of strictly non-parametric tools like frequency analyses and rank-order methods. While these are still useful techniques in our toolboxes, that time has passed, and now we have access to a wide range of tools that allow us to extend simple parametric and non-parametric tools to relax or change distributional assumptions. We will discuss these throughout the book, but we need to look at the underlying distributions that govern our decisions about which of these tools to use. So, this week well look at a few probability distributions that correspond to sampling distributions we frequently encounter in biology. To wrap-up, we will use this new information to talk about how we calculate descriptive statistics such as means and standard deviations from samples. "],["5.2-probability-distributions-in-r.html", "5.2 Probability distributions in R", " 5.2 Probability distributions in R R has a number of built-in distribution types, and there are random-number generators associated with most or all of these that will allow us to take random samples from a distribution (like picking numbers out of a hat!). This is useful for data simulation, but is also helpful for us to learn about probability distributions and how their parameters affect the shape, spread, scale, location, etc. of those distributions. We will briefly discuss concepts like skew because of how they can help us think about the assumptions that we are making (or breaking!) in the models that we use. For this class, we will focus on one major family of distributions and then zero in on a few distributions within this family that you are guaranteed to encounter throughout your career. "],["5.3-exponential-family.html", "5.3 Exponential family", " 5.3 Exponential family Most or all of the distributions we will use for this class come from the exponential family of distributions. The exponential family is very flexible, and whether you know it or not, it includes most of the distributions one might use to describe every day phenomena. It includes most of the probability distributions with which you are familiar, and many more. Just ask this very reliable Wikipedia entry. Oh, lets face it, you were going there anyway, I just cut out the Google step. Take a look at the table at the bottom of this Wikipedia page just to get an idea of how many distributions are included within the exponential! Holy cow! Were not going to look at all of these in this class- I just want you to be aware that this is a huge family of specific distributions. Distributions that well focus on in this chapter: 1. Continuous distributions Normal (Gaussian) Lognormal Beta Uniform 2. Discrete distributions Bernouli Binomial Multinomial Poisson Negative binomial "],["5.4-continuous-distributions.html", "5.4 Continuous distributions", " 5.4 Continuous distributions The normal distribution This is one distribution with which most of you have at least some nodding acquaintance. It is the classic bell curve that college students once dreaded in upper-level courses. I dont know if its a thing anymore. Go Google it. The normal distribution is defined by two parameters: The mean (\\(\\mu\\)) The variance (\\(\\sigma^2\\)) Lets take a look at what the normal distribution looks like. Well start with a special one called the standard normal (or z) distribution. The standard normal is a normal distribution with a mean of zero and a variance of 1. This one is really cool because the standard deviation (\\(\\sigma\\)) is the square-root of the variance, and in this special case \\(\\sqrt{1} = 1\\), so the variance and standard deviation are equal! And because of this property, and other normal distribution can be converted to a standard normal using z-standardization, which well talk about later. How exciting is that? First, take a sample from a normal distribution: samp &lt;- rnorm(n = 10000, mean = 0, sd = 1) Now, plot a histogram using the sick new skills you got in Chapter 4 p &lt;- ggplot() + geom_histogram(aes(samp), binwidth = 1) + scale_x_continuous(limits=c(-7,7), expand = c(0, 0)) + scale_y_continuous(expand = c(0, 1)) + xlab(&quot;Value&quot;) + ylab(&quot;Count&quot;) + theme_classic() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) print(p) Pretty! Because this sample is from a continuous distribution, we might actually wish to represent this distribution with a probability density function. You can think of this as R calculating the relative probability of an given value. It implies a continuous surface, rather than the discrete bins like the histogram. In reality it doesnt matter because at best we chop continuous distributions into tiny little bins when we do things like integrals, and R bases binwidth in histograms off a density function anyway (aaaaah!). Density plots are a new one for us, so lets try them out. If you scroll back and forth, youll notice that the code below is basically identical to the histogram above except for labels and scales. We just replaced the histogram geometry (geom_histogram) with a density-based geometry (geom_density). Here, we use fill = 1 to trick R into filling the area under the line because we have no grouping variables. By default, this is interpreted as 'black', so we add an alpha channel for transparency. p &lt;- ggplot() + geom_density(aes(samp), alpha = .1, fill = 1) + scale_x_continuous(limits = c(-7,7), expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + xlab(&quot;Value&quot;) + ylab(&quot;Density&quot;) + theme_classic() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) print(p) Excellent use of ggplot() to make a figure that looks like the clunky base graphics. Maybe you can improve on it in the homework assignment? We can change the parameters of the standard normal to change both the location and the scale of our distribution. The influence of changing the mean, or average, on the location of a distribution is perhaps obvious. But, the influence of variance may be less intuitive, so lets have a look! Create two more random samples, one with a larger sd and one with a smaller sd, to see how this changes the shape of the distribution: samp2 &lt;- rnorm(n = 1e4, mean = 0, sd = 2) samp3 &lt;- rnorm(n = 1e4, mean = 0, sd = .5) Lets put them in a data frame with samp so theyre easy to plot. We combine all three random samples into one column called Value. Then, we create a column to hold the standard deviation used in each sample (Sigma). If we make that one into a factor, we can use the Sigma columns to plot the samples as separate lines by tweaking our plotting code. normals &lt;- data.frame( Value = c(samp, samp2, samp3), Sigma = factor( c( rep(1, length(samp)), rep(2, length(samp2)), rep(.5, length(samp3)) ) ) ) Next, we can just add these to the plot to compare the sampling distributions. This time, we tell R to fill the area under our lines based on sample ID with a default color scheme by saying fill = Sigma in our ggplot() call. We also added color = Sigma to make the lines the same default colors. Remember, you can specify your own. p &lt;- ggplot(data = normals, aes(x = Value, group = Sigma, fill = Sigma, color = Sigma)) + geom_density(adjust = 1.5, alpha = .4) + scale_x_continuous(limits = c(-7, 7), expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + xlab(&quot;Value&quot;) + ylab(&quot;Density&quot;) + theme_classic() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) print(p) The blue polygon in the plot above shows a distribution with greater variance than our z distribution (green). The red polygon shows a distribution with a smaller variance. Hopefully this helps demonstrate how variance influences the scale of the distribution. 5.4.1 The lognormal distribution The lognormal distribution is a probability distribution that assumes our random variable is normally distributed on the log scale. This assumption allows us to incorporate skew into the normal distribution and change the location and scale of the normal distribution by transforming the parameters (\\(\\mu\\) and \\(\\sigma\\)) onto the log scale. This is one of the more common data transformations that you will run into, e.g.: We log-transformed the data to achieve normality. One of the other reasons for that is that all values (positive or negative) transformed from the log to the real scale are positive, so it helps prevent us from making negative predictions about phenomena or variables that cant be less than zero. Lets take a look at how changes to the mean change the location of this distribution: # Create random samples from log-normal # distributions with different means samp1 &lt;- rlnorm(n=1e4, mean=0, sd=1) samp2 &lt;- rlnorm(n=1e4, mean=1, sd=1) samp3 &lt;- rlnorm(n=1e4, mean=2, sd=1) # Put them in a data frame with the values # of the means used to create them lognormals &lt;- data.frame( Value = c(samp, samp2, samp3), X_bar = factor( c( rep(0, length(samp)), rep(1, length(samp2)), rep(2, length(samp3)) ) ) ) Now you can plot these using the code above with a couple of modifications to show how the mean of the log-normal distribution influences the location. p &lt;- ggplot(data = lognormals, aes(x = Value, group = X_bar, fill = X_bar, color = X_bar)) + geom_density(adjust = 1.5, alpha = .4) + scale_x_continuous(limits =c(0, 50), expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + xlab(&quot;Value&quot;) + ylab(&quot;Density&quot;) + theme_classic() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) print(p) You can see that the relative scale of these three distributions is similar, but the location shifts to the right on our x-axis as the value of X_bar (the mean) increases. Note also how this affects kurtosis. 5.4.2 The beta distribution The beta distribution is a probability distribution that is constrained to the interval [0, 1]. But, it is incredibly flexible in its parameterization, and as a result is very useful for stochastic simulation of variables on the probability scale, such as survival. The parameters of the beta distribution are \\(\\alpha\\) and \\(\\beta\\), or commonly a and b or shape 1 and shape 2 in R. Within this distribution, \\(\\alpha\\) pushes the distribution to the right (toward 1), and \\(\\beta\\) pushes the distribution back toward the left (toward 0). The relative magnitude of \\(\\alpha\\) and \\(\\beta\\) determine the location, shape, and scale of the probability distribution for our random variable. When \\(\\alpha\\) and \\(\\beta\\) are equal, and greater than 1, the beta distribution looks like a normal distribution within the interval [0, 1]. Lets take a look: # Simulate random values from 3 different beta distributions # so we can compare them samp1 &lt;- rbeta(n=1e4, shape1=50, shape2=50) samp2 &lt;- rbeta(n=1e4, shape1=50, shape2=100) samp3 &lt;- rbeta(n=1e4, shape1=500, shape2=250) # Put them in a data frame with the values # of the means used to create them. I am # using &quot;theta&quot; because often that is how we # refer collectively to a group of parameters betas &lt;- data.frame( Value = c(samp, samp2, samp3), theta = factor( c( rep(&#39;a = 50, b = 50&#39;, length(samp)), rep(&#39;a = 50, b = 100&#39;, length(samp2)), rep(&#39;a = 500, b = 250&#39;, length(samp3)) ) ) ) And then, we can plot them just like we did above. Copy and paste it - change what you need. Isnt code great?. Just dont forget to change the scale and the data in the plotting code! p &lt;- ggplot(data = betas, aes(x = Value, group = theta, fill = theta, color = theta)) + geom_density(adjust = 1.5, alpha = .4) + scale_x_continuous(limits =c(0, 1), expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + xlab(&quot;Value&quot;) + ylab(&quot;Density&quot;) + theme_classic() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), panel.grid = element_blank() ) print(p) Play around with these to see what kind of cool shapes you can make and where they are located within the range between zero and one. "],["5.5-discrete-distributions.html", "5.5 Discrete distributions", " 5.5 Discrete distributions Discrete probability distributions are useful for situations in which our random variable of interest can only take specific values within the interval of interest. For example, this might include age, counts, pass/fail, or any number of conceivable categories. As a result, these require a slightly different treatment of probability as a discrete, rather than continuous phenomenon. (Think back to our histogram that we started with in this chapter.) 5.5.1 Bernoulli The Bernoulli distribution is a special case of the binomial distribution with a single trial (see below for clarification). Bernoulli outcomes are those for which the variable we are measuring can take on one of two values: a one or a zero. This distribution is useful for visualizing processes such as coin flips, yes/no responses, live/dead endpoints in lab studies, and a number of other very interesting phenomena. The Bernoulli distribution has a single parameter: the probability of success, but the number of successful outcomes is also governed by sample size: n, which R calls size because n was already taken. We can simulate data from a Bernoulli distribution in one of two ways in R. The old-school way of doing this was to draw from a binomial with a single trial. Here we randomly draw a single sample from a binomial with a single trial, and a 50% chance of success. Well use the example of hatching chicken eggs with some probability of success. If you are boring, you can think about flipping coins, too! Well start with one chicken egg that has a 50% chance of successfully hatching (probability of success = 0.50). rbinom(n=1, size=1, prob=.5) ## [1] 1 There is also a function called rbern in the Rlab package that simplifies this for the specific case of a Bernoulli. Lets do it again with that function: # Hatch one egg with 50% success rate rbern(n = 1, prob = .5) ## [1] 0 Or we could hatch a whole bunch of eggs: # Hatch ten eggs, each with p = 0.5 rbern(n = 10, prob = .5) ## [1] 1 0 0 0 1 1 1 1 0 1 Then, we could even count how many of those were successful. Do you remember how to do that? There are several different ways. Youll have to come up with one for the homework assignment (hint: see Chapter 2). 5.5.2 Binomial The binomial distribution is pretty similar to the Bernoulli distribution. In fact, the Bernoulli is just a special kind of binomial. The binomial includes a parameter called \\(N\\) (size in R) which corresponds to a number of trials per sample. We assume that this is 1 in the case of Bernoulli. In most cases in biology, it will suffice to use the Bernoulli, but for modeling we will want to understand the binomial for things like random stratified designs and nested models that rely on the use of binomial distribution. Later in your career, you might even get into cool models that estimate \\(N\\) as a latent state to estimate population size (for example). Plus, using the binomial is way faster and can be more precise for certain regression applications [okay, that one should probably have a citation, but this is The Worst Stats Text eveR, so go Google it]. To sample data from a binomial distribution, we can use rbinom from base R. In this example we tell R that we want 10 samples (n) from a binomial distribution that has 10 trials (size) and a probability of success (prob) of 0.5. This is like hatching ten eggs from each of ten chickens instead of just one chicken laying ten eggs. # Take a random draw of 10 samples # from a binomial distribution with 10 trials # and probability of success equal to 0.50 rbinom(n=10, size=10, prob=0.5) ## [1] 4 5 5 4 5 7 4 3 5 6 Remember as you look through these that your numbers should look different than mine (at least most of the time) because these are being generated randomly. 5.5.3 Multinomial The multinomial distribution is a further generalization of the Binomial and Bernoulli distributions. Here, there are one or more possible categorical outcomes (states), and the probability of each one occurring is specified individually but all of them must sum to one. The categories are, in this case, assumed to be a mutually exclusive and exhaustive set of possible outcomes. We can use the multinomial distribution to randomly sample from categories (imagine our response variable is a categorical variable, like the names of the students in this class). To do this, we need to read in the s_names.csv file from our data folder that is definitely in your working directory (remember to set your working directory first). Read in the data file with stringsAsFactors = FALSE for purposes of demonstrating with categorical variables (not factors). s_names &lt;- read.csv(&#39;data/s_names.csv&#39;, stringsAsFactors = FALSE) Next, lets assign the variable name in s_names to a vector for simplicity. name &lt;- s_names$name Then, we can assign a uniform probability of drawing any given name if we divide one by the number of names. # Calculate probability of getting a given # name based on the length of the vector prob_each &lt;- 1 / length(name) # Repeat this probability for each # name in our vector of names probs &lt;- rep(prob_each, times = length(name)) probs ## [1] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 ## [7] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 ## [13] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 ## [19] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 ## [25] 0.03846154 0.03846154 This shows us that the probability of drawing any of the individual names is {r prob_each}. Now, we can sample from a multinomial distribution using our objects. Here we are taking 5 samples from the distribution, each time we sample there is only one trial, and we are sampling with the 26 probabilities above. Have a look: rmultinom(n=5, size=1, prob=probs) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 ## [3,] 0 0 0 0 0 ## [4,] 0 0 0 0 0 ## [5,] 0 1 0 0 0 ## [6,] 0 0 0 0 0 ## [7,] 0 0 0 0 0 ## [8,] 0 0 0 0 0 ## [9,] 0 0 0 0 0 ## [10,] 0 0 0 0 0 ## [11,] 0 0 0 0 0 ## [12,] 0 0 0 0 0 ## [13,] 0 0 0 0 0 ## [14,] 0 0 1 1 0 ## [15,] 0 0 0 0 0 ## [16,] 1 0 0 0 1 ## [17,] 0 0 0 0 0 ## [18,] 0 0 0 0 0 ## [19,] 0 0 0 0 0 ## [20,] 0 0 0 0 0 ## [21,] 0 0 0 0 0 ## [22,] 0 0 0 0 0 ## [23,] 0 0 0 0 0 ## [24,] 0 0 0 0 0 ## [25,] 0 0 0 0 0 ## [26,] 0 0 0 0 0 WHOA a matrix??!!! What does it all mean? Take a step back, breathe, and think about this. The rows in this matrix are you and your classmates. If we took one random sample from the multinomial distribution, it would look like this: # Take a single sample from # the list of student names rmultinom(n=1, size=1, prob=probs) ## [,1] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 0 ## [6,] 0 ## [7,] 0 ## [8,] 0 ## [9,] 0 ## [10,] 0 ## [11,] 0 ## [12,] 0 ## [13,] 0 ## [14,] 0 ## [15,] 0 ## [16,] 0 ## [17,] 0 ## [18,] 0 ## [19,] 0 ## [20,] 0 ## [21,] 0 ## [22,] 0 ## [23,] 0 ## [24,] 1 ## [25,] 0 ## [26,] 0 Here, we pulled a single sample from the distribution, and probability of sampling a given individual was 0.04 (1/26). If it makes it easier, we can put your names next to it: cbind(name, rmultinom(n=1, size=1, prob=probs)) ## name ## [1,] &quot;Ava&quot; &quot;0&quot; ## [2,] &quot;Dillon&quot; &quot;0&quot; ## [3,] &quot;Delaney&quot; &quot;0&quot; ## [4,] &quot;Manolo&quot; &quot;0&quot; ## [5,] &quot;Sarah&quot; &quot;0&quot; ## [6,] &quot;Shannon&quot; &quot;0&quot; ## [7,] &quot;Olivia&quot; &quot;0&quot; ## [8,] &quot;Ebony&quot; &quot;0&quot; ## [9,] &quot;Julia&quot; &quot;0&quot; ## [10,] &quot;Davi&quot; &quot;0&quot; ## [11,] &quot;Gabrielle&quot; &quot;0&quot; ## [12,] &quot;Jordan&quot; &quot;0&quot; ## [13,] &quot;Tayler&quot; &quot;1&quot; ## [14,] &quot;Summer&quot; &quot;0&quot; ## [15,] &quot;Leah&quot; &quot;0&quot; ## [16,] &quot;Christine&quot; &quot;0&quot; ## [17,] &quot;Ashley&quot; &quot;0&quot; ## [18,] &quot;Katherine&quot; &quot;0&quot; ## [19,] &quot;James&quot; &quot;0&quot; ## [20,] &quot;Emily&quot; &quot;0&quot; ## [21,] &quot;Cassidy&quot; &quot;0&quot; ## [22,] &quot;Maximillion&quot; &quot;0&quot; ## [23,] &quot;Sierra&quot; &quot;0&quot; ## [24,] &quot;Kyle&quot; &quot;0&quot; ## [25,] &quot;Diana&quot; &quot;0&quot; ## [26,] &quot;Amanda&quot; &quot;0&quot; Now, if I was calling on you randomly in class, after 10 questions, the spread of people who would have participated in class might look like this (or whatever you got - remember, it is random): cbind(name, rmultinom(n=10, size=1, prob=probs)) ## name ## [1,] &quot;Ava&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [2,] &quot;Dillon&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; ## [3,] &quot;Delaney&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [4,] &quot;Manolo&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [5,] &quot;Sarah&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [6,] &quot;Shannon&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [7,] &quot;Olivia&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [8,] &quot;Ebony&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [9,] &quot;Julia&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [10,] &quot;Davi&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [11,] &quot;Gabrielle&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; ## [12,] &quot;Jordan&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; ## [13,] &quot;Tayler&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [14,] &quot;Summer&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [15,] &quot;Leah&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [16,] &quot;Christine&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [17,] &quot;Ashley&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [18,] &quot;Katherine&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [19,] &quot;James&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [20,] &quot;Emily&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [21,] &quot;Cassidy&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [22,] &quot;Maximillion&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [23,] &quot;Sierra&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [24,] &quot;Kyle&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [25,] &quot;Diana&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [26,] &quot;Amanda&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; Taking this one step further, we could just draw a name and stop looking at these ugly (no but really they are awesome!) matrices: name[which(rmultinom(n=1, size=1, prob=probs)&gt;0)] ## [1] &quot;Katherine&quot; And now we have a way to randomly select an individual based on a multinomial distribution. What fun! 5.5.4 Poisson The Poisson distribution is used for counts or other integer data. This distribution is widely used (and just as widely misused!) for its ability to account for a large number of biological and ecological processes in the models that we will discuss this semester. The Poisson distribution has a single parameter, \\(\\lambda\\), which is both the mean and the variance of the distribution. So, despite its utility, the distribution is relatively inflexible with respect to shape and spread. Fun fact: this distribution was made widely known by a Russian economist to predict the number of soldiers who were accidentally killed from being kicked by horses in the Prussian army each year. It is named, however, after French mathematician Siméon Denis Poisson. [fails to provide citations for any of this] Take a look at how the distribution changes when you change \\(\\lambda\\), and you will get an idea of how this one works. It is probably the most straightforward of any weve considered. hist(rpois(n=1e4, lambda=100), main=&#39;&#39;) Well set it aside for now because it often fails us (or our data fail it, I suppose). 5.5.5 The negative binomial distribution Okay, this one can be a little difficult to wrap your head around but its an important one for us to know about. So, we will spend a little extra time setting this one up to try and be clear. Often, folks start out thinking that theyre going to use a Poisson distribution and they end up collecting with data that do not conform to the relative inflexibility of that single-parameter distribution. Where they end up usually tends to be a negative binomial in a best case (well talk about challenges associated with lots of zeros later in the book). For the purpose of this class, we are not going to dive into the mechanics of the negative binomial distribution, but we do need to know what it looks like and why we might need it. One useful way to conceptualize the negative binomial is how long does it take for some event to occur? For example, we might ask how long it takes a fish to start migrating, how long it takes a sea turtle to recover in a rehabilitation center, how long it will take for a terminal patient to expire (ooh, thats dark), or how frequently we see the expression of a gene of interest. These kinds of questions are asked in aptly named time-to-event models that rely on the variance structure of the negative binomial. In the context of these kinds of questions, the negative binomial is a discrete probability distribution (and not a continuous distribution) because the time component of the distribution is actually a series of independent Bernoulli trials (holy crap!). For example: if we want to know how many days it will take for a turtle to recover from an injury, what we are really doing is asking on each day until recovery, Is today the day? Then, we flip a coin and find out. So, each day in this example is a Bernoulli trial. Another way to think about this is the number of failures occurring in a sequence before a target number of sucesses is achieved. For the classical parameterization: We will start by looking at how many failures are observed before one success in a sequence of Bernoulli trials. With probability of succes equal to 0.95, it doesnt take long and most of the probability mass is near zero, with a couple of stragglers further out. # Take a random sample from the negative binomial Value &lt;- rnbinom(1e4, size=1, prob=.95) # Make a histogram of it with ggplot ggplot() + geom_histogram( aes(x = Value) ) If we decrease probability of success in each trial to 0.25, we see more failures on average before we reach success. Most of the time, it still takes less than 5 trials to reach a success, but some times it takes much longer. # Take a random sample from the negative binomial Value &lt;- rnbinom(1e4, size=1, prob=.25) # Make a histogram of it with ggplot ggplot() + geom_histogram( aes(x = Value) ) And, if we increase the number of successes that we use for our criterion, or target, then it spreads the distribution out even further. # Take a random sample from the negative binomial Value &lt;- rnbinom(1e4, size=10, prob=.25) # Make a histogram of it with ggplot ggplot() + geom_histogram( aes(x = Value) ) Now, because of its properties, the negative binomial is also useful for number of other applications that have nothing to do with interpretting the results of repeated binomial trials. Specifically, it has been widely used to represent Poisson-like processes in which the mean and variance are not equal (e.g., overdispersion). This has seen a lot of application in the field of ecology, especially for overdispersed count data. Here, we draw 10,000 random samples from a negative binomial distribution with a mean of 10 and an overdispersion parameter of 1. The overdispersion parameter is called size because this is an alternative parameterization that is just making use of the relationships between existing parameters of the negative binomial. Its easy to grasp how the mean changes the location of the distribution. # Take a random sample from the negative binomial Value &lt;- rnbinom(1e4, mu = 10, size = 1) # Make a histogram of it with ggplot ggplot() + geom_histogram( aes(x = Value), bins = 20 ) But, note how the overdispersion parameter changes things if you run the following code: # Take a random sample from the negative binomial Value &lt;- rnbinom(1e4, mu = 10, size = 1000) # Make a histogram of it with ggplot ggplot() + geom_histogram( aes(x = Value), bins = 20 ) A more intuitive way (I think) to work with the negative binomial in R is by using the MASS package. In this parameterization, we use the mean and the dispersion parameter explicitly so it makes more sense: # Take a random sample from the negative binomial Value &lt;- rnegbin(1e4, mu = 10, theta = 1000) # Make a histogram of it with ggplot ggplot() + geom_histogram( aes(x = Value), bins = 20 ) The results are pretty much identical. Just two different naming systems for the parameters. "],["5.6-sample-statistics.html", "5.6 Sample statistics", " 5.6 Sample statistics In this section, we will learn how to derive the parameters of the normal distribution using a few different methods in R. We will use this opportunity to re-introduce the parameters as moments of the distribution so we can talk about what we mean by confidence intervals. We also will introduce a couple of different methods for calculating moments of a distribution. Specifically, we will look at how to derive 5.6.1 Moments about the mean Sounds fancy, huh? Here they are, like a bandaid: Zeroth moment This is the sum of the total probability of the distribution 1.00, always First moment The mean We will look at a few ways to calculate this Second moment The variance As with the mean, we will examine a couple of options for calculating Third moment Skew We wont calculate for this class, but we have discussed, and this parameter contributes to the location/spread of the distribution (how far left or right the peak is) Fourth moment Kurtosis Similarly, we wont cover the calculation, but this is another moment that we may have discussed with respect to departure from a z distribution in the normal 5.6.2 Estimating parameters of the normal distribution from a sample The tools demonstrated below can be used for most of the probability distributions that have been implemented in R, and we could go on and on forever about them. But, for the sake of our collective sanity we will walk through the tools available using the normal distribution alone. Most of the time this will suffice because our objective in understanding other distributions is really just so that we can use them to assume asymptotic normality in response variables (with transformations) or parameter distributions (with link functions) later on anyway. 5.6.2.1 Method of moments estimator The moments of the normal distribution are well defined, and you are probably familiar with how to calculate a mean (average) already. See if you can rearrange this in a way that makes sense with how you know to calculate a mean and a variance! Start by simulating a variable with a known mean and standard deviation. Well pretend that we are simulating cold temperatures here: # Take a random sample from a normal # with a mean of 20 and a standard # deviation of 2 test_norm &lt;- rnorm(1e4, 20, 2) First, well estimate it by making our own function: # Write the function # First, define a function by name norm.mom = function(x){ # Calculate mean x_bar = (1/length(x)) * sum(x) # Calculate variance sigma2 = (1/length(x)) * sum((x-x_bar)^2) # Return the calculations return(c(x_bar, sigma2)) } # Test the function norm.mom(test_norm) ## [1] 19.94024 3.98326 Because this one is so common, R has built-in estimators that rely on the exact solution provided by the formulas for the first two moments of the normal distribution: mean(test_norm) ## [1] 19.94024 var(test_norm) ## [1] 3.983658 Wow, that was a lot less code. That is the beauty of having these functions available. How do these compare to the answers returned by our function if you scroll back up? 5.6.2.2 Maximum likelihood estimator R also has built-in maximum likelihood estimators that provide an exact solution to the first two moments of the normal distribution. These are available through the MASS package. fitdistr(test_norm, &#39;normal&#39;) ## mean sd ## 19.94023785 1.99581051 ## ( 0.01995811) ( 0.01411251) Only one problem here: R doesnt report the second moment! It reports the square root of the second moment: the standard deviation! Finally, lets write our own function and maximize the likelihood with the optim() function in R. # Define the function normal.lik = function(theta, y){ # The starting value for # mu that we provide mu = theta[1] # The starting value for # sigma2 that we provide sigma2 = theta[2] # Number of observations in the data n = nrow(y) # Compute the log likelihood of the # data (y) using the likelihood # function for the normal distribution # given the starting values for our # parameters (contained in the vector &#39;theta&#39;) logl = -.5*n*log(2*pi) -.5*n*log(sigma2)-(1/(2*sigma2))*sum((y-mu)**2) return(-logl) } Now, we use the optim function to maximize the likelihood of the data (technically by minimizing the -2*log[likehood]) given different values of our parameters (mu and sigma2). To get started, we need to take a guess at what those parameters could be. (Yes, we know they are mu = 20 and sd = 2) optim(c(20, 4), normal.lik, y=data.frame(test_norm)) ## $par ## [1] 19.940005 3.982909 ## ## $value ## [1] 21099.89 ## ## $counts ## function gradient ## 47 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The pieces are in pars here (right where we told R to put them!). We can also make the output into an object and call the parts by name: # Make it into an object est = optim(c(0, 1), normal.lik, y=data.frame(test_norm) ) ## Warning in log(sigma2): NaNs produced ## Warning in log(sigma2): NaNs produced ## Warning in log(sigma2): NaNs produced ## Warning in log(sigma2): NaNs produced ## Warning in log(sigma2): NaNs produced ## Warning in log(sigma2): NaNs produced Look at the structure Ill be damned, its a list! Hey, we learned about those! str(est) ## List of 5 ## $ par : num [1:2] 19.94 3.98 ## $ value : num 21100 ## $ counts : Named int [1:2] 95 NA ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;function&quot; &quot;gradient&quot; ## $ convergence: int 0 ## $ message : NULL And, here are the estimates: # Both est$par ## [1] 19.936142 3.977272 # The mean est$par[1] ## [1] 19.93614 # The variance est$par[2] ## [1] 3.977272 There you have it, a couple of different ways to calculate parameters of the normal distribution using a couple of different approaches each. 5.6.3 Quantiles and other descriptive statistics There are a number of other ways we might like to describe this this (or any) sampling distribution. Here are a few examples that we will work with this semester. # Here is the median, or 50th percentile median(test_norm) ## [1] 19.93941 # The 95% confidence limits quantile(test_norm, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 15.97333 23.81833 # Interquartile range (Q1 and Q3) quantile(test_norm, probs = c(0.25, 0.75)) ## 25% 75% ## 18.58380 21.29847 # Range of sample range(test_norm) ## [1] 12.29180 27.31528 "],["5.7-next5.html", "5.7 Next steps", " 5.7 Next steps Here, we have explored some of the probability distributions that we use to describe samples (data) that we collect from the real world. In Chapter 6 we will explore how these sampling distributions can be used for statistical inference before diving into applied statistical analyses for the remainder of the book. Hopefully the order of things is starting to make some sense! If not, wellthis is The Worst Stats Text eveR. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
