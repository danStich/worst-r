[
["17-Chapter17.html", "17 Bayesian Logistic regression", " 17 Bayesian Logistic regression “Life or death” is a phrase we reserve for situations that are not normal. Coincidentally, life or death is also a binary variable, and therefore it’s residuals are also not normal. Will these zebra mussels live or die? That will be our next adventure, but for that we need the generalized linear model (GLM). "],
["17-1-intro17.html", "17.1 Introduction", " 17.1 Introduction In this chapter, we will follow along with the case example from Chapter 12 that examined a behavioral choice made by Atlantic salmon smolts while they migrated from streams to the ocean. We’ll work with some packages from the tidyverse, rstan, and rstanarm, and we’ll use the StillwaterChoice data from the class data folder. You can go ahead and load those whenever you are ready to get started. library(tidyverse) library(rstan) library(rstanarm) You can also set up some options for Stan right up front. Here I am telling rstan not to compile a model everytime I run one that is already compiled, and I am setting an option that tells R to detect the number of virtual “cores” on my computer so it can run models in parallel to speed things up. This means that instead of sampling all my Markov Chains serially one at a time I can sample each of them on a separate process on my computer. rstan::rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) "],
["17-2-logistic-17.html", "17.2 Binary (logistic) regression", " 17.2 Binary (logistic) regression Here, we will duplicate the analysis from Chapter 12 using a Bayesian framework with the rstanarm package. We’ll note some differences and some similarities as we go along. Read in the data file. choice &lt;- read.csv(&quot;data/StillwaterChoiceData.csv&quot;) Look at the first few rows of data: head(choice) ## path year hatchery length mass date flow ## 1 0 2010 1 176 57 118 345 ## 2 0 2005 1 205 101 128 1093 ## 3 0 2010 1 180 56 118 345 ## 4 0 2010 1 193 74 118 345 ## 5 0 2005 1 189 76 128 1093 ## 6 0 2010 1 180 65 118 345 17.2.1 Data Explanation The full data explanation is provided in Chapter 12.4 and will not be repeat it here. Go check it out if you need a reminder of what data are in there. 17.2.2 Data analysis We are going to use the 1/0 binary data to estimate the effects of a number of covariates of interest on the probability that an individual fish used the Stillwater Branch for migration in each year of this study using logistic regression. If our response, path is a binary variable where 1 = Stillwater and 0 = mainstem for each fish 1 to n, we can think of p(Stillwater Branch) as: \\[p(Stillwater) = \\frac{\\sum{path}}{n}\\] and the logit of p(Stillwater Branch) can assumed to be normally distributed with a mean of . \\[logit(p) = Normal(\\mu, \\sigma^{2})\\] Now that we know we are doing more or less the same thing as we were for linear models, let’s move on with fitting the model. First, since we are interested in the fixed effects of year, and not the linear trend through time, we need to convert year to factor. choice$year &lt;- as.factor(choice$year) Now, if we want to test hypotheses about the influences of explanatory variables on the probability of using the Stillwater Branch, we could make models to represent those hypotheses. For example, if we wanted to test whether flow had a significant influence on path across years, then we could build a model that looks like this: flow_mod &lt;- stan_glm(path ~ year + flow, family = binomial(link = &quot;logit&quot;), data = choice) We could make another model that investigates effects of length on path choice instead of flow: len_mod &lt;- stan_glm(path ~ year + length, family = binomial(link = &quot;logit&quot;), data = choice) Or a model that includes both with an annoying name: flow_len_mod &lt;- stan_glm(path ~ year + flow + length, family = binomial(link = &quot;logit&quot;), data = choice) We could look at these individually to determine variable-level significance using approaches demonstrated in Chapter 16.9. First, let’s define a slightly more complex set of models based on a priori combinations of explanatory variables. Note that this is pretty much identical to how we do this for models fit with glm() except now we are using stan_glm() to fit the models! # Make an empty list to hold the models mods &lt;- list() # Now, add models to the list. Stop and think about what each one means. mods[[1]] &lt;- stan_glm(path ~ year + hatchery + length + flow, family = binomial, data = choice) mods[[2]] &lt;- stan_glm(path ~ year + flow, family = binomial, data = choice) mods[[3]] &lt;- stan_glm(path ~ year + hatchery, family = binomial, data = choice) mods[[4]] &lt;- stan_glm(path ~ year + length, family = binomial, data = choice) mods[[5]] &lt;- stan_glm(path ~ year + length + hatchery, family = binomial, data = choice) mods[[6]] &lt;- stan_glm(path ~ year + length + flow, family = binomial, data = choice) mods[[7]] &lt;- stan_glm(path ~ year + hatchery + flow, family = binomial, data = choice) Next, give the models some names using the formulas for each of the models. Remember: models are stored as list objects in R, and each of those list objects (models) has names. We can reference those names using the $ notation, and from there we can access the actual model formula. The third element of this formula object contains the explanatory variables!! Just like glm(). Whoa! We can extract the formula for each model (which is an element in the mods list) using a for loop to assign them one at a time. Here, we are assigning the ith formula to be the name of the ith element in the list mods. Nifty. Note that this is pretty much identical to how we do this for models fit with glm()! # Assign the formula for each of the models as the name for (i in 1:length(mods)) { names(mods)[i] &lt;- as.character(mods[[i]]$formula)[3] } Now, we use the loo package to make a model selection table like we did in Chapter 16.8: # Load the library library(loo) # Extract the log-likelihood matrices log_liks &lt;- lapply(mods, log_lik) # Now apply the loo() function to each # model to get elpd_loo loos &lt;- lapply(log_liks, loo) # Finally, we can compare them with loo_compare() mod_table &lt;- loo_compare(loos) Nice. 17.2.3 Interpreting the results This proceeds the same way for GLM as it does for linear models until we get to making predictions of the response based on our best model. Our model selection table is an object in R (right?), and we can reference that object using $ notation, matrix notation [ , ], or by calling rownames() to get the name for each of the models. Let’s use this approach to get the best model from our candidate set. # Print the table mod_table ## elpd_diff se_diff ## year + flow 0.0 0.0 ## year + length + flow -0.2 1.2 ## year + hatchery + flow -0.2 1.5 ## year + hatchery + length + flow -1.1 1.5 ## year + hatchery -2.2 3.0 ## year + length -2.8 2.8 ## year + length + hatchery -3.1 3.0 # Look at the structure just to show that it is, indeed, an object: str(mod_table) ## &#39;compare.loo&#39; num [1:7, 1:8] 0 -0.203 -0.232 -1.12 -2.203 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:7] &quot;year + flow&quot; &quot;year + length + flow&quot; &quot;year + hatchery + flow&quot; &quot;year + hatchery + length + flow&quot; ... ## ..$ : chr [1:8] &quot;elpd_diff&quot; &quot;se_diff&quot; &quot;elpd_loo&quot; &quot;se_elpd_loo&quot; ... Look at the rownames of the table. These rownames are the index for each of our models as they appear in the mods object, and we can use the index to reference objects inside of the mods list… rownames(mod_table) ## [1] &quot;year + flow&quot; &quot;year + length + flow&quot; ## [3] &quot;year + hatchery + flow&quot; &quot;year + hatchery + length + flow&quot; ## [5] &quot;year + hatchery&quot; &quot;year + length&quot; ## [7] &quot;year + length + hatchery&quot; The rowname for the best model (the one at the top of the table) is year + flow. Print the model to see it as follows. Notice the back-ticks (``) around the model name to deal with spaces. print( mods$`year + flow`, digits = 3) ## stan_glm ## family: binomial [logit] ## formula: path ~ year + flow ## observations: 759 ## predictors: 7 ## ------ ## Median MAD_SD ## (Intercept) -2.977 0.732 ## year2006 -0.519 0.602 ## year2009 0.277 0.455 ## year2010 -0.002 0.613 ## year2011 -0.788 0.407 ## year2012 -0.744 0.497 ## flow 0.002 0.001 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg Great! Let’s save this to an object so we can work with it a little easier. best_mod &lt;- mods$`year + flow` We can get an understanding of the explanatory power of our model using a Bayesian R2 (described by Gelman et al. 2018). By default, this function will return one estimate of the R2 for each iteration of the model, so we should get 4,000 estimates of R2 here. r_squared &lt;- bayes_R2(best_mod) mean(r_squared) ## [1] 0.05010344 Meh, about average for an ecological study, but that doesn’t mean it is good. What we are really after here is just a good idea of what proportion of fish use this route and whether it can be reasonably well related to discharge, so we’ll keep pressing along to see if we have that. Let’s wrap our interpretation of results by looking at coefficient estimates as indicators of statistical significance. First, we could extract just the parameters for ease: coeffs &lt;- data.frame( as.matrix(best_mod) ) The coeffs object is now a dataframe with 4,000 posterior samples (rows) of each model coefficient (columns). Then, we can determine whether zero or any other value of interest is included within the credible range of values for each of these coefficients. If zero falls outside the credible range for any of the parameters, we can conclude that the effect was significant. For categorical variables, this means groups are different. For continuous explanatory variables, it means that the slope is different from zero (flat). Pivot the coefficients into long form first so they are grouped for plotting, and then we’ll summarize the posteriors. coeffs_stacked &lt;- coeffs %&gt;% pivot_longer(cols = everything(), names_to = &quot;coeff&quot;, values_to = &quot;val&quot; ) %&gt;% group_by(coeff) You can calculate means and 95% Credible intervals for plotting: post_summary &lt;- coeffs_stacked %&gt;% summarize( fit = mean(val), lwr = quantile(val, 0.025), upr = quantile(val, 0.975) ) We can plot these really easily to see whether any of the parameters overlap zero: ggplot(post_summary, aes(x = coeff, y = fit, color = coeff, fill = coeff)) + geom_point(size = 2) + geom_segment(aes(xend = coeff, y = lwr, yend = upr), lwd=1) You can get a similar plot using the built-in plot() method for the fitted model object a lot more easily like so. By default, this method plots 95% high density (credible) intervals for each of the posteriors. plot(best_mod) 17.2.3.1 Categorical effects Regardless of the method we use, we see there is a lot of variability between years and there is reasonable support to suggest that this variability is significant. How can we tell this? We don’t have access to p-values like we do in MLE or OLS anymore because of how the likelihood is expressed in these models. Instead, we can do group-wise comparisons to determine specific differences but this becomes cumbersome when we have many levels. Remember, that the null hypothesis for factors under an ANOVA- or ANCOVA-like analysis is just that any two group means differ. In this case, it looks like 2011 and 2009 are the most different since 2011 has lower probability of using Stillwater Branch than 2005 (Intercept) and 2009 is greater. If we want to compare year-specific means, we’ll need to make predictions on the logit scale and then convert to the real scale where we can actually derive the difference between years arithmetically for the entire posteriors. Make predictions from the model on the logit scale, and convert to the real scale using the invlogit() function from the rstanarm package: p_stillwater_2009 &lt;- invlogit( coeffs$X.Intercept. + coeffs$year2009 + coeffs$flow * mean(choice$flow)) p_stillwater_2011 &lt;- invlogit( coeffs$X.Intercept. + coeffs$year2011 + coeffs$flow * mean(choice$flow)) And now you can calculate the difference (note that this is for the average flow). difference &lt;- p_stillwater_2009 - p_stillwater_2011 If we are interested in whether the true difference is equal to zero (null) then we can calculate quantiles: quantile(difference, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 0.03098018 0.18955307 In this case, there is at least a 95% chance that the probability of smolts using the Stillwater Branch varies between 2009 and 2011, so we can conclude that the effect is significant at \\(\\alpha\\) = 0.05. We can also plot a histogram of the differences really easily now that it is just a vector in R: ggplot() + geom_histogram(aes(x = difference), binwidth = 0.01) + xlab(&quot;Difference in p(Stillwater) between 2009 and 2011&quot;) + ylab(&quot;Count&quot;) + scale_y_continuous(limits = c(0, 450), expand = c(0, 0)) + theme_bw() + theme( axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 3), ) 17.2.4 Continuous effects Interpreting significance and direction of continuous effects is a little more straightforward. In this case, we just need to determine whether the posterior estimate of the coefficient for continuous explanatory variables differs from zero or another value of interest. quantile(coeffs$flow, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 0.0005353392 0.0028944054 It was really hard to see in the coefficients plot due to differences in scale, so we can make a histogram of this one to visualize that difference as well. 17.2.5 Making predictions The first thing to remember here is that we have used a link function to estimate this model, so we cannot use the same method as we did for linear models to make predictions about our response from the model coefficients. The second thing to remember here is that by definition we have used an invertible link function to estimate this model so the previous statement is a lie and we actually can use the same method as before to make predictions about our response from the model coefficients. We just need to add an extra step so that we can invert our predictions about the expected value of Y with respect to X. Confused? Yeah, it’s a little confusing. As always an example always goes a long way… We can make predictions from our best model pretty much the same way we did in Chapter 12.4.4. # Make predictions from the best model logit_preds &lt;- data.frame( predict(best_mod, type = &quot;link&quot;, se.fit = TRUE) ) # Calculate confidence intervals as 1.96 * standard error logit_preds$lwr &lt;- logit_preds$fit + 1.96 * logit_preds$se.fit logit_preds$upr &lt;- logit_preds$fit - 1.96 * logit_preds$se.fit # Invert the link function real_preds &lt;- apply(logit_preds, 2, invlogit) # Combine the predictions with the original data choice_preds &lt;- data.frame(choice, real_preds) Go ahead and have a look at the logit_preds and real_preds objects to make sure you understand what we just did. Now, we can finish by plotting our predictions: ggplot(choice_preds, aes(x = flow, y = fit, fill = year)) + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = year), alpha = 0.25) + geom_line(aes(color = year)) + xlab(expression(paste(&quot;Flow ( &quot;, m^3, &quot;\\u00b7&quot;, s^-1, &quot;)&quot;))) + theme_bw() + theme(panel.grid = element_blank()) You can see that, in general, there is a relatively low probability of an individual fish using the Stillwater Branch, but we see increasing probability of using that route with increasing flow across years. "],
["17-3-next12.html", "17.3 Next steps", " 17.3 Next steps Here we have demonstrated similarities between the GLM and the models with which we have worked previously. You should realize now that the linear models we’ve been using are really just a special kind of GLM that uses a “normal” or “Gaussian” error distribution. If we think about what kind of data we actually have, this can open up lots of other “non-normal” options without scaring the life out of us! Hopefully, logistic regression is now a useful tool in your statistical hardware box. Finally, you should also appreciate that we have multiple methods (OLS, MLE, Bayesian) for estimating these models and they all work in similar ways with slightly different approaches. Next, we’ll look at how to keep extending this GLM framework for analysis of count data in Chapter 18. "]
]
