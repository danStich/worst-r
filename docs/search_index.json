[["7-Chapter7.html", "7 Linear models", " 7 Linear models Yeah, I know this is the picture from Chapter 4. I only have like five pictures. This is the Worst Stats Text eveR! But, both of the graphs in this picture are just applications of linear regression, which is one kind of linear model, which is also called the general linear model. Introduction In this chapter, we will introduce a class of statistical tools known collectively as linear models. This class of tools includes such examples as analysis of variance (ANOVA), linear regression and correlation, and by extension includes n-way ANOVA, multiple linear regression, and analysis of covariance (ANCOVA). Later this semester, we will see that these models can be extended even further to include generalized linear models, generalized linear mixed models, multivariate techniques and even machine learning algorithms. Linear models are, therefore, the gateway into the rest of the world of statistics. We will focus primarily on parametric applications this week and next. The over-arching theme for this week is that any of these methods can be expressed as the formula for a line, which is how they got their names (oh, snap!). We will start with ANOVA because it is analogous to many of the methods that weve already discussed. However, it is important to recognize that this is just a special case of the linear model. This will help you think about how we test statistical assumptions, test hypotheses, and communicate results of models. Because we are now entering into the realm of the rest of statistics we also need to start talking the talk in addition to walking the walk, so we will practice how to write methods sections for these tests and how to report the results. In reality, once you are comfortable using a couple of functions in R, writing up the methods and results is more challenging than fitting models. library(tidyverse) "],["7.1-anova.html", "7.1 Analysis of variance (ANOVA)", " 7.1 Analysis of variance (ANOVA) We will use some of the built-in datasets in R this week to demonstrate our analyses and show how to communicate the methods and the results of our statistical inference. Analysis of variance is typically used when we have a continuous dependent variable (y, response) and a categorical independent variable (x, explanatory) containing three or more groups. As with the t-test, we assume that the error (variance) within groups is normal (well discuss in detail in Chapter 8). Most of the math behind ANOVA is basically the same as a t-test, but we add a couple steps for the third group, and now it essentially becomes the same thing as an F test (var.test() from Chapter 6[#Chapter6]) on three groups. In fact, the t-test, the F test, and one-way anova are all pretty much the same thing! mind == blown 7.1.1 One-way analysis of variance The number of grouping variables used in ANOVA confers different fancy naming conventions. The simplest of these contains a single grouping variable (e.g. treatment or year) and is referred to as one-way ANOVA. In theory, these models can be extended to include any number n of grouping variables (e.g. treatment and year) and are commonly referred to as n-way ANOVA. Lets start by loading the PlantGrowth dataset in R: data(PlantGrowth) PlantGrowth is a dataframe with 30 observations of two variables. The first variable weight describes plant growth (in units of mass), and the second variable group contains control (ctrl) and two treatment groups (trt1 and trt2) for individual plants. Have a look, as always: str(PlantGrowth) ## &#39;data.frame&#39;: 30 obs. of 2 variables: ## $ weight: num 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ... ## $ group : Factor w/ 3 levels &quot;ctrl&quot;,&quot;trt1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Lets begin by using a one-way ANOVA to determine if the mass of plants differs between groups in PlantGrowth. In practice, this is very easy. First of all, though, we would report our methods something like this: We used a one-way analysis of variance (ANOVA) to estimate the effects of treatment group on the mass (g) of plants assuming a Type-I error rate of \\(\\alpha\\) = 0.05. Our null hypothesis was that all group means were equal (H0: \\(\\mu_{ctrl} = \\mu_{trt1} = \\mu_{trt2}\\)). Therefore, if any one of the means is not equal to the others, then we reject the null hypothesis. You can fit an ANOVA using either the aov() function or the lm() function in base R. I prefer to use lm() for two reasons. First, there is output from lm() that I dont get with aov(). Second, the lm() function is the one well use for linear regression, multiple regression, and analysis of covariance. This reminds us that these models are all special cases of the glorious, over-arching group of general linear models in Chapter 8 and will help us develop a standard workflow moving forward. # Fit the model model &lt;- lm(weight~group, data=PlantGrowth) # Print the model object to the console model ## ## Call: ## lm(formula = weight ~ group, data = PlantGrowth) ## ## Coefficients: ## (Intercept) grouptrt1 grouptrt2 ## 5.032 -0.371 0.494 Wow, that is dangerously easy to do. But, this output is not very useful for getting the information we need if you dont already know what you are looking at. What we get here is essentially just one part of the information that we would like to (should) report. Well proceed with a more standard ANOVA table for now using the anova() function: # Save anova table to an object plant_nova &lt;- anova(model) # Have a look at the goodness print(plant_nova) ## Analysis of Variance Table ## ## Response: weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 3.7663 1.8832 4.8461 0.01591 * ## Residuals 27 10.4921 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Okay, this is really what we needed in order to evaluate our null hypothesis: an ANOVA table with a break down of the residuals, mean squared errors, and test statistic(s). We interpret the test statistic and the p-value the same way we did in Chapter 6 when we did t-tests and Wilcox tests. And, we can now say: We found that the treatment had a significant effect on plant weight (ANOVA, F = 4.846, df1 = 2, df2 = 27, p = 0.0159). We can also calculate the R2 value for the ANOVA, which is a statistic used to describe the amount of variation in the data explained by the model relative to the total variation in the data set. More correctly, we are actually comparing the sum of squared errors for the model we fit (SSB) to the total sum of squares (SST = SSB + SSE). For what its worth, this is a super useful statistic for getting the big-picture perspective on whether your model is useful or crap. You calculate it pretty easily from the anova() output like as: \\[R^2 = \\frac{SSB}{SST}\\] or Why wouldnt you do this in R? # Look at the names of the anova() output # We want the &quot;Sum Sq&quot; bit names(plant_nova) # Here is the sum of squares for the model # Have to use back-ticks for spaces, sigh ssb &lt;- plant_nova$`Sum Sq`[1] # And the sum of squares total is the sum # of the column in this case sst &lt;- sum(plant_nova$`Sum Sq`) # Some quick division, and we get... R2 &lt;- ssb/sst Have a look: print(R2) ## [1] 0.2641483 Now, we can say that our treatment effect explained about 26% of the variation in the data. The rest is a combination of error and unexplained variation in the data that might require further investigation. The only problem here is that this is an awful lot of work to get something that should be really easy to do in R. And, we still dont know how weight varied between groups. We just know that at least one group is different from the other. Thankfully, the default output of summary() for linear models fit with lm() does a lot of this for us. # Print the summary of the model summary(model) ## ## Call: ## lm(formula = weight ~ group, data = PlantGrowth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.0320 0.1971 25.527 &lt;2e-16 *** ## grouptrt1 -0.3710 0.2788 -1.331 0.1944 ## grouptrt2 0.4940 0.2788 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 Thats better, we get some useful information here. First of all, we get the value of the test statistic, the df, and the p-value for the model. We also get the \\(R^2\\) for the model, 0.26, as part of the default output. But, what if we want to know more about how treatment affected weight? Like, which groups are different? Can we use the p-values reported in the column Pr(&gt;|t|) to infer group-wise differences? The quick answer is sometimes. The Coefficients chunk of this output can help us with inference in simple situations, and it really is the key to making predictions from our models (see Chapter 10). Remember, most model output in R is stored as lists, so we can extract the coefficients table like this if we look at `names( summary(model) ) to find what we want: coeffs &lt;- data.frame( summary(model)$coefficients ) Okay, what is going on here? This looks nothing like the output we got from anova(). The coeffs object is a dataframe with columns for mean coefficient estimates, the standard error of those estimates, t-statistic, and p-value. We are actually not going to worry about the p-values here for a hot minute. Lets focus on the Estimate column first. There are three values here. Each of these represents one of the factor levels in the group variable in PlantGrowth. They are assigned in ascending alpha-numeric order based on the data. The first level (ctrl) is assigned as the (Intercept) or base level against which all others are compared. In this sense, the (Intercept) coefficient is an estimate of the mean value of weight for the group called ctrl. Do a quick check: # Calculate and print mean weight # for the group ctrl in PlantGrowth PlantGrowth %&gt;% filter(group == &#39;ctrl&#39;) %&gt;% summarize(avg = mean(weight)) ## avg ## 1 5.032 As you can see, the prediction from the ANOVA is identical to the group mean estimated directly from the data. The coefficients for grouptrt1 and grouptrt2 can be thought of adjustments to the (Intercept) coefficient, or the difference between the mean of ctrl and trt1 or trt2. If the Estimate for grouptrt1 or grouptrt2 is negative, then the mean for that group is less than ctrl and if it is positive, the mean for the group is greater than ctrl. If we wanted to calculate the mean weight of the trt1 and trt2 groups, we would add them to the (Intercept) coefficient like this: # Assign model coefficients to objects ctrl &lt;- coeffs$Estimate[1] trt1 &lt;- coeffs$Estimate[2] trt2 &lt;- coeffs$Estimate[3] # Calculate group means for trt1 and trt2 # from the model trt1_prediction &lt;- ctrl + trt1 trt2_prediction &lt;- ctrl + trt2 print(c(trt1_prediction, trt2_prediction)) ## [1] 4.661 5.526 If you calculate the means for these groups directly from the data youll see that these values are identical to the mean weight of the trt1 and trt2 groups. In Chapter 10 we will examine how to estimate confidence intervals around these estimates and make predictions from the model that include our uncertainty. But for that, well need to talk about a little bit of math and were dealing with enough right now already! Finally, the p-values associated with trt1 and trt2 indicates whether each group is significantly different from ctrl. In the case of the intercept, the p-value simply tells us whether the mean weight of ctrl is significantly different from zero. A fundamentally dull question - of course the intercept is different from zero because plants cant possibly have a weight of zero or less. This is the first time we really need to think about the differences between our statistical null hypotheses and our biological null hypotheses. If we want to do further comparisons between groups (other than just comparing trt1 and trt2 to ctrl by themselves), then we need to add on a little post-hoc testing to find out which groups differ. We can use a pair-wise comparison to test for differences between factor levels. Because this essentially means conducting a whole bunch of t-tests, we need a way to account for our repeated Type-I error rate, because at \\(\\alpha\\) = 0.05 we stand a 1 in 20 chance of falsely rejecting the null even if it is true. One tool that allows us to make multiple comparisons between groups while adjusting for elevated Type-I error is the Tukey HSD (honest significant difference) test. This test makes comparisons between each pairing of groups while controlling for Type-I error. Essentially, this makes it harder to detect differences between groups but when we do we are more sure that they are not spurious (Honest significant difference, say it with me). Sound confusing? At least its easy to do in R. We need to recast our model as an aov object in R first to use the TukeyHSD() functionthis is essentially the same thing as the lm function, but in a different wrapper (literally) that allows us to access the info in a different way. It would be a fine default function for doing ANOVA if we werent interested in going any further with linear models. TukeyHSD( # The function that does the Tukey test aov( # A wrapper for lm objects model # The model that we ran above ) ) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = model) ## ## $group ## diff lwr upr p adj ## trt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711 ## trt2-ctrl 0.494 -0.1972161 1.1852161 0.1979960 ## trt2-trt1 0.865 0.1737839 1.5562161 0.0120064 This report shows us exactly how weight differs between each pair of treatment groups. Here, we see that the only significant difference (p adj &lt; 0.05) occurs between trt2 and trt1. For the readers, and for us, it may be easier to see this information displayed graphically: ggplot(PlantGrowth, aes(x = group, y = weight, color = group, fill = group)) + geom_boxplot(alpha = 0.15, width = .25) + geom_jitter(width = 0.1) + xlab(&quot;Treatment group&quot;) + ylab(&quot;Weight (g)&quot;) + labs(fill = &quot;Group&quot;, color = &quot;Group&quot;) + theme_bw() + theme(axis.title.x = element_text(vjust=-1), axis.title.y = element_text(vjust=3) ) In addition to what we wrote before, we can now say something along the lines of: We found that the mass of plants in the trt2 group (5.5 \\(\\pm\\) 0.4 g) was significantly greater than plants in the trt1 group (4.7 \\(\\pm\\) 0.8 g; Tukey HSD, p = 0.012). However, we failed to detect differences in mass between plants in the control group (5.0 \\(\\pm\\) 0.6 g) and trt1 (p = 0.39) or trt2 (p = 0.20). 7.1.2 Two(n)-way ANOVA Next, well step up the complexity and talk about cases for which we have more than one grouping variable and some kind of numeric response. In these cases, we can use a two-way ANOVA (or n-way depending on number of factors) to examine effects of more than one grouping variable on the response. Here, we will use a data set describing differences in the mass of belly- button lint collected from males and females of three species of apes. # Read in the data: lint &lt;- read.csv(&#39;data/lint.txt&#39;) 7.1.2.1 Main effects model Now we can fit a model to the data. This will work the same way as for the one-way ANOVA above, but this time we will add more terms on the right hand side of the equation. We will start by looking at the main effects model for this data set. What is a main-effects model? This model assumes that the response of interest, in this case the mass of belly button lint, lintmass, is affected by both species and gender, and that within species the effect of gender is the same. For example, the mass of belly button lint could be greater in one species compared to others, and if there is a difference between sexes we would expect that trend to be the same across species (e.g., boys always have more lint than girls - sorry guys, its probably true!). # Fit the model and save it to an object lint.model&lt;- lm(lintmass~species + gender, data = lint) # Look at the summary of the model fit summary(lint.model) ## ## Call: ## lm(formula = lintmass ~ species + gender, data = lint) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5792 -0.9021 0.0875 0.8448 2.3917 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.5458 0.6133 25.346 &lt; 2e-16 *** ## speciesSpecies 2 -3.4375 0.7512 -4.576 0.000183 *** ## speciesSpecies 3 -1.8750 0.7512 -2.496 0.021414 * ## genderMale 4.9083 0.6133 8.003 1.16e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.502 on 20 degrees of freedom ## Multiple R-squared: 0.8096, Adjusted R-squared: 0.781 ## F-statistic: 28.35 on 3 and 20 DF, p-value: 2.107e-07 # Print the anova table anova(lint.model) ## Analysis of Variance Table ## ## Response: lintmass ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 47.396 23.698 10.499 0.0007633 *** ## gender 1 144.550 144.550 64.041 1.16e-07 *** ## Residuals 20 45.143 2.257 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As you can see, the output for the model is much the same as for the one-way ANOVA. The only real difference is that we have more than one grouping variable here. We conclude that there is a significant difference in lintmass between species (F = 10.50, df = 2, p &lt; 0.05) and between genders (F = 64.01, df = 1, p &lt; 0.05). As before, we can make a quick boxplot and overlay our raw data to visualize these differences: lint %&gt;% ggplot(aes(x = species, y = lintmass, color = gender, fill = gender)) + geom_boxplot(alpha = 0.10) + geom_point(position=position_jitterdodge(.1)) + xlab(&#39;Species&#39;) + ylab(&#39;Lint mass&#39;) + theme_bw() + labs(fill = &quot;Gender&quot;, color = &quot;Gender&quot;) + theme(axis.title.x = element_text(vjust=-1), axis.title.y = element_text(vjust=3), panel.grid = element_blank() ) Hopefully after seeing these results you are now starting to realize how important a few well-placed figures and tables can be for clearly communicating the results of your research (even if it is about belly-button lint). The math for making predictions becomes a little more complicated once we add a second grouping variable. Even the numbers of pair-wise comparisons can become overwhelming in a simple situation like this. Therefore, well hold off on digging too much deeper into the math until next week. 7.1.2.2 Interaction terms The n-way ANOVA is the first kind of model we have used in which it is possible to consider interactions between two or more factors. An interaction occurs when the effects of two or more factors are not additive. This means that the effect of gender might change for different species. For example, let us consider the following scenario in the lint data. Perhaps we hypothesize that lint accumulation in the belly buttons of females differs in pattern from males due to social grooming patterns and sex-specific behavioral patterns favoring females in only certain species. As a result, we might expect that gender and species could have some kind of non-additive effect on lintmass in these apes such that there are significant, sex-specific differences only in some species. To test this, we would use the following: # Fit a new model that includes an interaction, signified by &#39;*&#39; lint.modeli &lt;- lm(lintmass~species * gender, data=lint) # Summarize the model summary(lint.modeli) ## ## Call: ## lm(formula = lintmass ~ species * gender, data = lint) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.525 -0.750 0.050 1.019 1.875 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.9250 0.7404 20.159 8.40e-14 *** ## speciesSpecies 2 -2.2500 1.0471 -2.149 0.0455 * ## speciesSpecies 3 -1.2000 1.0471 -1.146 0.2668 ## genderMale 6.1500 1.0471 5.874 1.46e-05 *** ## speciesSpecies 2:genderMale -2.3750 1.4808 -1.604 0.1261 ## speciesSpecies 3:genderMale -1.3500 1.4808 -0.912 0.3740 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.481 on 18 degrees of freedom ## Multiple R-squared: 0.8335, Adjusted R-squared: 0.7873 ## F-statistic: 18.03 on 5 and 18 DF, p-value: 1.898e-06 # Print an ANOVA table for the model anova(lint.modeli) ## Analysis of Variance Table ## ## Response: lintmass ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 47.396 23.698 10.8079 0.0008253 *** ## gender 1 144.550 144.550 65.9253 1.983e-07 *** ## species:gender 2 5.676 2.838 1.2943 0.2984104 ## Residuals 18 39.468 2.193 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Alas, in the case of the lint model, this interaction is not significant, so we lack the evidence we would need to say that lint accumulation changes differently between genders within species. "],["7.2-simple-linear-regression.html", "7.2 Simple linear regression", " 7.2 Simple linear regression We have now considered the case of what to do when we have a numerical response and categorical explanatory variable(s) with any number of groups or grouping variables. But, what if we have both a numerical response and numerical explanatory variables? Fear not, there is a stat for that! Now we are entering the realm of correlation and regression. Next week, well show that ANOVA is, in fact, just a special kind of regression. When we fit a linear regression model, we are trying to explain relationships between some response of interest (dependent variable y) and one or more explanatory (independent) variables, x. As with all linear models the goal of regression analysis is, in its simplest sense, to fit a line through all of the points in bivariate space that minimizes the distance between the points and a line of the form: \\[y = mx + b\\] That ought to look familiar! In the case of statistics, we usually represent the formula for a line like this: \\[Y_i = \\beta_0 + \\beta_i X_i\\] We are ignoring an important part of these statistical models for now. In most cases, though, we will be estimating a parameter for the intercept and one parameter for each explanatory variable of interest. 7.2.1 Simple linear regression Since most folks are probably more familiar with linear regression than with ANOVA whether they know it or not, well jump right into this one with an example using the swiss data. These data are for fertility and infant mortality rates as related to a number of socio-economic indicators. Take a moment to look at them: data(swiss) You can see the description of these data by looking at the help file for the data set as always. Have look on your own: ?swiss Now, lets get cracking. Well start by fitting a simple model and then build complexity. Fit a model that relates fertility to education level. Notice that this looks exactly the same as the call to lm for the ANOVAs above? Thats because they are the same thing and people have been lying to you your whole life. Perhaps it took reading The Worst Stats Text eveR to learn it? If so, I aplogize for your misfortune. # Fit the model and assign it to a named object fert_mod &lt;- lm(Fertility ~ Education, data = swiss) # Summarize the model summary(fert_mod) ## ## Call: ## lm(formula = Fertility ~ Education, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.036 -6.711 -1.011 9.526 19.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79.6101 2.1041 37.836 &lt; 2e-16 *** ## Education -0.8624 0.1448 -5.954 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.446 on 45 degrees of freedom ## Multiple R-squared: 0.4406, Adjusted R-squared: 0.4282 ## F-statistic: 35.45 on 1 and 45 DF, p-value: 3.659e-07 The (Intercept) term in this summary is the y-intercept from our formula for a line and the Education coefficient is the slope of the line. Our intercept tells us that mean Fertility (y) is about 79.6 when Education (x) is zero. Note that this interpretation does not change even if we did not observe an education of zero in the data - something to think about in the weeks to come. The p-value for the intercept just tells us that this value is significantly different from zero (snores). The p-value for the Education coefficient tells us that the slope of the line is also significantly different from zero. Because this number is negative, we know that there is an inverse relationship between Education and Fertility. In other words, more highly educated individuals have fewer children. You can tell this is an inverse relationship because of the minus sign in front of the coefficient for Education. We know that the relationship is significant because of the small p-value and corresponding significance codes. We explained a little more than 40% of the variability in the response with this one explanatory variable if we look at the R2 value that is returned (well work with the Multiple R-squared by default). This is as far as the summary goes for linear regression for now. That is, we dont need the ANOVA table to assess significance any more because we have no factors - just continuous variables. What we end up with in this summary are the coefficients that can be used to describe the line that passes through the data and minimizes the residual errors (thats the part we ignored above). WHAT?? Lets explain this by actually looking at the data and plotting our model over the top of it. First, well use the built-in predict() function to create a trend line and a prediction interval. Well dig deeper into how to do this in Chapter 10. # Make predictions from the fitted model object using observed data predicted_fertility = predict(fert_mod, interval = &#39;confidence&#39;) # Add these to the swiss data swiss_pred &lt;- cbind(swiss, predicted_fertility) Now, we can plot the raw data as a scatterplot and add our model estimates over the top. You should notice that the confidence interval is much wider at high values of Education because there are few data points and thus more uncertainty in that part of the data. # Sets up data and aesthetics ggplot(swiss_pred, aes(x = Education, y = Fertility)) + # Adds raw data as points geom_point(colour = &#39;blue&#39;, fill = &#39;blue&#39;, alpha = 0.3, size = 2) + # Adds regression line geom_line( aes(y = fit), size = 1) + # Adds 95% confidence interval geom_ribbon(aes(ymin = lwr, ymax = upr), color = &#39;purple&#39;, alpha = .2) + # Adds sweet style tweaks of your choosing theme(legend.position = &quot;none&quot;) Again, dangerously easy. "],["7.3-multiple-linear-regression.html", "7.3 Multiple linear regression", " 7.3 Multiple linear regression We can, of course, extend this to include multiple continuous explanatory variables of interest just as we did with ANOVA for multiple categorical explanatory variables! Here is an example to whet your appetite. Lets say we want a multiple regression model that includes both Education and Catholic? multiple_mod &lt;- lm(Fertility ~ Education + Catholic, data = swiss) summary(multiple_mod) ## ## Call: ## lm(formula = Fertility ~ Education + Catholic, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.042 -6.578 -1.431 6.122 14.322 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 74.23369 2.35197 31.562 &lt; 2e-16 *** ## Education -0.78833 0.12929 -6.097 2.43e-07 *** ## Catholic 0.11092 0.02981 3.721 0.00056 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.331 on 44 degrees of freedom ## Multiple R-squared: 0.5745, Adjusted R-squared: 0.5552 ## F-statistic: 29.7 on 2 and 44 DF, p-value: 6.849e-09 Or if we really want to get crazy with the hot sauce: full_mod &lt;- lm( Fertility ~ Agriculture + Examination + Education + Catholic, data = swiss ) summary(full_mod) ## ## Call: ## lm(formula = Fertility ~ Agriculture + Examination + Education + ## Catholic, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.7813 -6.3308 0.8113 5.7205 15.5569 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 91.05542 6.94881 13.104 &lt; 2e-16 *** ## Agriculture -0.22065 0.07360 -2.998 0.00455 ** ## Examination -0.26058 0.27411 -0.951 0.34722 ## Education -0.96161 0.19455 -4.943 1.28e-05 *** ## Catholic 0.12442 0.03727 3.339 0.00177 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.736 on 42 degrees of freedom ## Multiple R-squared: 0.6498, Adjusted R-squared: 0.6164 ## F-statistic: 19.48 on 4 and 42 DF, p-value: 3.95e-09 "],["7.4-next7.html", "7.4 Next steps", " 7.4 Next steps During the next couple of weeks, well try to figure out a way to deal with this rats nest of different explanatory variables and how they are interpreted. But first, well talk about combining ANOVA and linear regression into a general linear model (analysis of covariance) in Chapter 8 and how to assess assumptions (Chapter 9) and communicate our results effectively (Chapter 10). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
