[["14-Chapter14.html", "14 Linear mixed models", " 14 Linear mixed models This is a big, fat yellow perch . It has grown large by eating invasive rusty crayfish (I made that up - Worst Stats Text eveR). Those rusty crayfish have grown large by eating in our local streams. If we had samples of crayfish from lots of different streams we could find out which rusty crayfish grow the fattest and how this translates to where they are having the biggest impacts. Its crayfish all the way down this time. "],["14.1-intro-14.html", "14.1 Introduction", " 14.1 Introduction This week we will talk about extending linear models and generalized linear models to include random effects in the model, thus resulting in the generalized linear mixed model or GLMM. The GLMM is actually the most generalized formulation of the linear models that we have been discussing now for the past several weeks. All linear models (GLM, ANCOVA, ANOVA, regression, t-tests, etc.) are special cases of the GLMM. As such, we can think of the GLMM as the framework within which we have been working for weeks now! We will start with examples of the linear mixed model (LMM) in this Chapter and extend these to include non-normal data when we dig into the GLMM in Chapter 15. In this chapter, well introduce some tools from the lme4 and merTools packages in addition to the usual suite of functions we need from the tidyverse. Youll need to install these before you can use them. Then, go ahead and load them up when you are ready to get started: library(tidyverse) library(lme4) library(merTools) "],["14.2-assumptions-lmm.html", "14.2 Assumptions of linear models", " 14.2 Assumptions of linear models OMG, why is this guy always talking about assumptions of linear models no matter what we do?! Just as we discussed last week, linear models are just a special case of the GLMM. That is, the linear model assumes a certain error distribution (the normal or Gaussian) that helps things work smoothly and correctly. During the last two weeks, we discussed how we can use link functions to relax the assumption of linear models with respect to normality of residuals and homogeneity of variances, as well as assumptions about the linearity of relationships between explanatory variables and responses of interest by using data transformation. This week, we continue to relax the underlying assumptions of linear models to unleash the true power of estimation in mixed effects models. This is essentially as far as the basic framework for linear modeling goes (with the exception of multivariate techniques), and all other cases (e.g. spatial and temporal autocorrelation regressions) are simply specialized instances of these models. Lets take another look at the assumptions of linear models. We will repeat the same mantra from the past few weeks. Here are the three assumptions that we explicitly use when we use linear models (just in case youve forgotten them): Residuals are normally distributed with a mean of zero Independence of observations (residuals) Colinearity Auto correlation of errors (e.g., spatial &amp; temporal) Homogeneity of variances Linear relationship between X and Y 14.2.1 Assumption 1: Normality of residuals Weve seen these before, but lets recap. For assumption 1, we are assuming a couple of implicit things: 1) The variable is continuous (and it must be if its error structure is normal), and 2) The error in our model is normally distributed. In reality, this is probably the least important assumption of linear models, and really only matters if we are trying to make predictions from the models that we make, or when we are in gross violation of the assumption. Of course, we are often concerned with making predictions from the models that we make, so we can see why this might be important. However, more often we are in extreme violation of this assumption in some combination with assumption 4 above to such a degree that it actually does matter. For example, a response variable that is binomial (1 or zero) or multinomial in nature cannot possibly have normally distributed errors with respect to x unless there is absolutely no relationship between X and Y, right? So, if we wanted to predict the probability of patients dying from some medical treatment, or the presence/absence of species across a landscape then we cant use linear models. This is where the link functions that we have been discussing really come into play. The purpose of the link function is to place our decidedly non-normal error structures into an asymptotically normal probability space. The other key characteristic of the link function is that it must be invertible, that way we can get back to the parameter scale that we want to use for making predictions and visualizing the results of our models. 14.2.2 Assumption 2: Independence of observations This time weve broken assumption 2 in two components: Colinearity and autocorrelation of errors. Remember that the manifestation of these problems has primarily been in the precision of our coefficient estimates so far. This leads to the potential for change in the Type-I/II error rates in our models, causing us to draw false conclusions about which variables are important. As we discussed earlier in the course we expect to see some colinearity between observations, and we can deal with balancing this in our modeling through the use of model selection techniques to reduce Type-I and Type-II error. During the past couple of weeks, we examined tools that help us determine whether or not colinearity is actually causing problems in our models that go beyond minor nuisances. As for the second part, autocorrelation, we briefly touched on formulations of the GLM in our readings that included auto-regressive correlation matrices to relax this assumption of linear models and improve the precision of parameter estimates. This week, we will further extend this to include random effects so we can account for non-independence in the observations, and correlation in the residual errors of explanatory variables that could otherwise cause issues with accuracy and precision of our estimates. We will continue to use model selection as a method for determining tradeoffs between information gain and parameter redundancy that results from colinearity between explanatory variables, as well as for hypothesis testing. 14.2.3 Assumption 3: Homogeneity of variances In past weeks, we looked at ways to reduce this issue by introducing blocking (categorical) variables to our models. Last week, we noted that this could be further mitigated through the use of weighted least squares and MLE within the GLM framework, which can be applied to a wide range of regression methods from linear models to GLMs and GLMMs. This week we will examine how we can use various formulations of the GLMM to account for heteroscedasticity in residual errors directly by including the appropriate error terms in our models. This essentially means that we can start to account for things like repeated measures, nested effects, and various other violations through the use of one toolnifty!! 14.2.4 Assumption 4: Linearity and additivity Weve already looked at a couple of ways to deal with violations of these assumptions such as data transformation and/or polynomial formulations of the linear model. We will continue to apply these concepts this week as we begin to investigate the GLMM as robust framework for analysis. "],["14.3-linear-mixed-models.html", "14.3 Linear mixed models", " 14.3 Linear mixed models We will start our explorations into GLMM by looking at the somewhat familiar case of normal data, whatever mythical meaning it may have. As with the relationship between ANOVA and GLM, we can say that the linear mixed model (LMM) is just a special case of the GLMM (hence the name), both of which belong to the group of multi-level or hierarchical models that house basically every kind of model we have looked at this semester. So, what is a mixed model? This is a model that, generally speaking, assumes at least one parameter of interest is drawn from a population of potential sample sets. We usually use these when we are dealing with repeated samples for some group or individual, or if we wish to account for some latent variable beyond our control (e.g., lake or year). The use of random effects allows us to remove extraneous noise (variance) from the study system by explicitlty accounting for it. This can improve both the accuracy and the precision of estimates to make hypothesis testing on other explanatory variables more robust. It also allows us to generalize our conclusions to a broader scope (e.g. any lake instead of lakes X, Y, and Z). Beyond these mundane uses, a multi-level approach to modeling allows for a great deal of flexibility in assumptions we make about the effects and associated errors in our model. We might assume within our model that effects are different between populations by assigning random intercepts and/or slopes. We can specify whether we think the influence of a continuous covariate is correlated with the starting point (correlated random slopes and intercepts). There are even rare cases when we might wish to examine random slopes with shared intercepts or vice versa. In Bayesian inference we can use information at higher levels of organization, like the North American Range of a species, to inform parameter estimation at lower levels, such as individual populations. The point here is that random effects on a given parameter need not be a nuisance for which we wish to account: it may be the very thing we wish to harness for inference, estimation, or prediction. As with so many things, these tools are often best investigated through the use of a worked example. Generally speaking, we want the grouping variable we use to specify random effects to contain a relatively large number of potential levels (usually &gt; 5, but often &gt; 10) as this tends to result in more accurate, and more precise parameter estimates. We will look at a case to start in which we use fewer for the sake of demonstration. "],["14.4-lmm-worked.html", "14.4 Worked example", " 14.4 Worked example Lets start by loading in a new data set. These data come from a preliminary study of rusty crayfish Faxonius rusticus density in various tributaries to the Susquehanna River. The data were collected as part of a summer research project by one of our former high-school interns at the SUNY Oneonta Biological Field Station . # Read in the data file # We are reading it in with the optional # argument because we don&#39;t want to deal # with code relicts if we start # dropping factor levels cray &lt;- read.csv(&quot;data/cray.csv&quot;, stringsAsFactors = FALSE) # Look at the data structure str(cray) ## &#39;data.frame&#39;: 964 obs. of 7 variables: ## $ date : chr &quot;6/28/2018&quot; &quot;6/28/2018&quot; &quot;6/28/2018&quot; &quot;6/28/2018&quot; ... ## $ waterbody: chr &quot;Susquehanna River&quot; &quot;Susquehanna River&quot; &quot;Susquehanna River&quot; &quot;Susquehanna River&quot; ... ## $ site : chr &quot;Cooperstown Dam&quot; &quot;Cooperstown Dam&quot; &quot;Cooperstown Dam&quot; &quot;Cooperstown Dam&quot; ... ## $ length : num 30.9 31.2 31.1 29.3 28.3 ... ## $ mass : num 7.18 6.85 6.75 6.46 6.14 5.99 5.58 4.95 4.85 4.81 ... ## $ loglength: num 3.43 3.44 3.44 3.38 3.34 ... ## $ logmass : num 1.97 1.92 1.91 1.87 1.81 ... # And, have a look at the first few lines of data head(cray) ## date waterbody site length mass loglength logmass ## 1 6/28/2018 Susquehanna River Cooperstown Dam 30.88 7.18 3.430109 1.971299 ## 2 6/28/2018 Susquehanna River Cooperstown Dam 31.18 6.85 3.439777 1.924249 ## 3 6/28/2018 Susquehanna River Cooperstown Dam 31.15 6.75 3.438814 1.909543 ## 4 6/28/2018 Susquehanna River Cooperstown Dam 29.28 6.46 3.376905 1.865629 ## 5 6/28/2018 Susquehanna River Cooperstown Dam 28.31 6.14 3.343215 1.814825 ## 6 6/28/2018 Susquehanna River Cooperstown Dam 30.55 5.99 3.419365 1.790091 This is a fairly straightforward data set. There are 964 observations of 7 variables. Each of the observations (rows) corresponds to a rusty crayfish that was collected and measured (length in mm and mass in g) at one of several sites on a given date. The variable catch is the total number caught by electrobugging over a given time (minutes). To compare density between sites, catch was divided by (time/60) to calculate catch per unit effort (cpue) as number of crayfish per hour. Therefore, observations of cpue, catch, and time correspond to unique date and site combinations, but length and mass represent unique individuals within site and date. Our primary objective in this study was to collect baseline data. But curiosity led us to explore variation in the condition of crayfish when we thought we were noticing skinnier crayfish in sites of higher density. Length-weight regressions are one tool that is commonly used to investigate changes in volumetric growth with increasing length. In the absence of a standardized condition metric such as that widely applied in fish populations, relative weight (Wr), we thought this might offer some insight into variability in condition. What follows is a steroid-injected version of the analysis we started with. Length-weight relationships in animals are generally parameterized by log-transforming length and mass. This is because the relationship between the two is exponential, or can be described using a power function (ouch, think back to intro bio). For a given unit increase in length, we expect mass to increase as an approximately cubic function of length. We can see this in our un-transformed data pretty cleanly: ggplot(cray, aes(x = length, y = mass)) + geom_point() The relationship depicted above can be expressed mathematically as: \\[W = aL^b,\\] and statistically as: \\[W_i = a {L_i}^b e^{\\epsilon_i},\\] where \\(W_i\\) is mass (weight) of individual \\(_i\\), \\(L_i\\) is length of the individual, \\(a\\) and \\(b\\) are the coefficient and exponent describing change in mass as a function of length, and \\(\\epsilon_i\\) is the multiplicative error term for each crayfish (residuals change with length). Here, \\(b\\) also has interpretation relative to allometry in growth patterns, where values of 3.0 indicate isometry, values below 3 indicate negative allometry, and values above 3 indicate positive allometry in the length-weight relationship. This means that at values much above 3, we would expect individuals to get heavier faster relative to their length at large sizes. We can certainly estimate this kind of relationship using nonlinear regression. But, nonlinear regression can get a little unstable due to inherent correlations between parameters, and the multiplicative error described above. So, it can be easier to log-transform both sides of the equation to make the relationship linear and achieve a constant error across the range of X (homoscedasticity). As a result, we generally linearize relationships like this to improve modeling whenever we can, in this case by taking the natural logarithm of both sides of the equation: \\[log(W_i) = log(a) + b \\cdot log(L_i) + \\epsilon_i\\] Now, that should look a whole lot like the linear models we have been talking about all semester. In fact, by substitution, we could say that: \\[Y = \\beta_0 + \\beta_1 \\cdot X_i + \\epsilon_i\\] where \\(Y\\) is log(mass), and \\(X\\) is log(length). Then, we just need to remember that \\(\\beta_0\\) is estimated on the log scale. We can take a look at how this looks in our data by plotting the transformed data. Start by log-transforming length and mass # Log transform length and mass cray$loglength &lt;- log(cray$length) cray$logmass &lt;- log(cray$mass) Plot the relationship. Note that only the names have been changed ggplot(cray, aes(x = loglength, y = logmass)) + geom_point() If nothing else, this tells me we need to go after more small cray next summer. For now, lets get rid of all crayfish weighing less than 1 g because the data are sparse down there and small values are known to cause some trouble in these kinds of models [still refusing to provide citations]. # Drop the measurements for crayfish &lt; 1 g cray &lt;- cray %&gt;% filter(mass &gt;= 1) Now, lets take a look at the residual diagnostics for a linear model that includes a group effect of site and (of course) length as explanatory variables that we will use to predict changes in our response of interest, mass. testmod &lt;- lm(logmass ~ loglength + site, data = cray) ggplot(testmod, aes(x = site, y = .resid, fill = site, color = site)) + geom_violin(alpha = 0.20, trim = FALSE) + geom_jitter(width = 0.1, alpha = 0.20) These look pretty good over all, but you should recognize at this point that we may have some concerns about whether or not variances are equal between groups, and whether observations at each site can really be considered independent (they cannot). Next, we will take a look at a few different ways to analyze these data using maximum likelihood estimation. Our goal here is to estimate the relationship between length and mass while accounting for inherent variability between sites. 14.4.1 Random-intercepts model First, we will analyze the data assuming that the intercepts for our linear model can vary between populations, but the relationship between length and mass is the same across all populations. This is a very common approach in many ecological and biological applications, as it often is the case that we are just trying to account for sampling design when we do this kind of analysis. This is really straightforward to do in R. First, load the lme4 package if you didnt do it at the start of the chapter. # Load the package library(lme4) Next, we fit the model. To do this we will use the lmer() function. Notice that now we have to add an argument to specify a random effect of site on the intercept 1. This is our way of telling R that we want to account for site-specific variability in the length-mass relationship. # Fit the model craymod &lt;- lmer(logmass ~ loglength + (1 | site), data = cray) Being responsible individuals, we now have a look at the residuals to make sure weve met assumptions of normality and homoscedasticity: # Get the residuals cray$resid &lt;- residuals(craymod) # Plot the residuals ggplot(cray, aes(x = site, y = resid, fill = site, color = site)) + geom_violin(alpha = 0.20, trim = FALSE) + geom_jitter(width = 0.1, alpha = 0.20) Thats looking about as good as we could hope for, so now lets go ahead and crack open the model summary. # Print the summary summary(craymod) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: logmass ~ loglength + (1 | site) ## Data: cray ## ## REML criterion at convergence: -980.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -8.0026 -0.5412 -0.0534 0.4781 6.2908 ## ## Random effects: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.001792 0.04234 ## Residual 0.018911 0.13752 ## Number of obs: 890, groups: site, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -7.99294 0.08031 -99.53 ## loglength 2.89236 0.02417 119.64 ## ## Correlation of Fixed Effects: ## (Intr) ## loglength -0.975 For now, we will skip the usual walkthrough of all the wonderful tidbits that R has to offer and cut right to the chase. We can see from the table for our fixed effects that we have successfully detected the relationship between length and mass (p &lt; 0.05), but this should come as no surprise based on the plot we saw and the fact that I already told you this is a well conserved biological relationship. We can see that the estimated sd for our intercept is fairly low, so we may not need to specify this as a random effect were we concerned about model complexity. Given that we are interested in this random effect, and that we (in this case) want to think of site as having been sampled from a broader population of sites, we will retain it in our model. From here, we could go on to make predictions across populations using our fixed intercept and slope, or we could use the population specific intercepts and the shared slope. First, lets make some predictions about the average relationship between length and mass in each site. This is still really easy to do using the built-in predict() methods for lmer() objects: # Make predictions from craymod log_preds &lt;- predict(craymod) # Undo the log-transform real_preds &lt;- exp(log_preds) # Combine them with the original data cray_preds &lt;- data.frame(cray, log_preds, real_preds) Now we can plot our predictions by treating this just like any other model: ggplot(cray_preds, aes(x = length, y = mass, color = site, fill = site)) + geom_point(alpha = 0.10) + geom_line(aes(y = real_preds), lwd = 1, alpha = 0.50) + xlab(&quot;Carapace length (mm)&quot;) + ylab(&quot;Mass (g)&quot;) This is helpful for identifying some of the differences between sites. In this case, it looks like we have slightly smaller crayfish in a handful of sites (Bailey Road, Colliersville, and Route 80) and the rest of them look pretty similar. It can be difficult to see how our model fits the data for individual groups. Remember, that we could address this by using a quick facet_wrap() in our plotting code: ggplot(cray_preds, aes(x = length, y = mass, color = site, fill = site)) + geom_point(alpha = 0.10) + geom_line(aes(y = real_preds), lwd = 1, alpha = 0.50) + facet_wrap(~site) + xlab(&quot;Carapace length (mm)&quot;) + ylab(&quot;Mass (g)&quot;) This lets us see how well our predictions fit the data from individual streams. But, we still dont really have a great way of looking at differences between groups if we are interested in those. And, we cant see the uncertainty in our predictions (e.g. the confidence bands we usually plot with these). Why is this? We do not have the technology. Basically, computing group-specific variances is conceptually and programmatically challenging. But, we can use some simulation methods to do this, and some of these have been implemented in newer versions of the lme4 package and related packages like merTools. IMO, if you are going to go through simulations just to approximate confidence intervals, you are probably interested in the group-level estimates as well, and you should really be thinking about Bayesian approaches at this point. But we wont talk about those in this book because it is The Worst Stats Text eveR. In the meantime, here is one way to compute site-specific variances for our predictions about the length-mass relationship so we dont have to feel like we havent finished the job! This example uses the predictInterval() function from the merTools package to demonstrate. # Simulate predictions from the relationship # stored in the model fit using our original data log_preds &lt;- predictInterval( merMod = craymod, level = 0.95, n.sims = 1000, stat = &quot;median&quot;, type = &quot;linear.prediction&quot;, include.resid.var = TRUE ) # Convert them to the real scale for plotting real_preds &lt;- apply(log_preds, 2, exp) # Combine predictions with the original data mer_preds &lt;- data.frame(cray, real_preds) Finally, lets add that beautiful uncertainty to our site-specific predictive plots that we made above! ggplot(mer_preds, aes(x = length, y = mass, color = site, fill = site)) + geom_point(alpha = 0.10) + geom_ribbon(aes(ymin = lwr, ymax = upr, color = NULL), alpha = .3) + geom_line(aes(y = fit), lwd = 1, alpha = 0.50) + facet_wrap(~site) + xlab(&quot;Carapace length (mm)&quot;) + ylab(&quot;Mass (g)&quot;) This may seem like a lot of work at first glance, but hopefully you do realize that it is about the same amount of code as making predictions from linear models and general linear models, and now we have introduced yet another way to dealing with violations of assumptions! Realize that these tools are still constantly being developed, and just a couple of years ago it took a lot more code to do this for mixed models than it does now. "],["14.5-next-14.html", "14.5 Next steps", " 14.5 Next steps This chapter provided a basic overview of why to use mixed models, how to analyze them, and how to use predictions and plots to determine what the results mean. These models are a potentially powerful tool that arguably are still under-utilized in biology and ecology. To crank up the power even more, we will generalize these models to include tools like logistic regression and count models in Chapter 15. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
